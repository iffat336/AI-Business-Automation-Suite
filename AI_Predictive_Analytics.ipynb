{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc5c6533",
   "metadata": {},
   "source": [
    "# üöÄ AI-Driven Predictive Analytics for Business Decision Making\n",
    "\n",
    "---\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This comprehensive notebook implements **two major AI-driven predictive analytics projects** designed to empower business decision-making:\n",
    "\n",
    "### üìä Project 1: Sales Forecasting Model\n",
    "Predict future sales performance using **Random Forest**, **Gradient Boosting (XGBoost)**, and **LSTM Deep Learning** models based on historical data, customer behavior, and seasonal trends.\n",
    "\n",
    "### üë• Project 2: Customer Churn Prediction  \n",
    "Predict customer churn using **Logistic Regression**, **Random Forest**, **XGBoost**, and **Deep Neural Networks** to help businesses retain customers by identifying at-risk clients.\n",
    "\n",
    "---\n",
    "\n",
    "**Skills:** Machine Learning, Deep Learning, Python, Data Analysis  \n",
    "**Tools:** scikit-learn, XGBoost, LightGBM, TensorFlow/Keras, matplotlib, seaborn, plotly  \n",
    "**Datasets:** Kaggle Sales Forecasting & Telco Customer Churn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bcb1e6",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff5aea56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully!\n",
      "NumPy: 2.4.2\n",
      "Pandas: 3.0.0\n",
      "TensorFlow: 2.20.0\n",
      "XGBoost: 3.2.0\n",
      "Scikit-learn: 1.8.0\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Import Required Libraries\n",
    "# ============================================================\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (mean_absolute_error, mean_squared_error, r2_score,\n",
    "                             accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             roc_auc_score, roc_curve, precision_recall_curve,\n",
    "                             confusion_matrix, classification_report, auc)\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# XGBoost & LightGBM\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Imbalanced Learning\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# TensorFlow/Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, BatchNormalization, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "tf.random.set_seed(RANDOM_STATE)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "print(f\"Pandas: {pd.__version__}\")\n",
    "print(f\"TensorFlow: {tf.__version__}\")\n",
    "print(f\"XGBoost: {xgb.__version__}\")\n",
    "print(f\"Scikit-learn: {__import__('sklearn').__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90bd05c",
   "metadata": {},
   "source": [
    "## 2. Create Output Directory for Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "979236b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Chart directories created:\n",
      "   üìÅ /Users/usama/Desktop/dummy/charts/sales_forecasting\n",
      "   üìÅ /Users/usama/Desktop/dummy/charts/customer_churn\n",
      "   üìÅ /Users/usama/Desktop/dummy/charts/comparison\n",
      "   üìÅ /Users/usama/Desktop/dummy/charts/dashboard\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Create Output Directories for Charts\n",
    "# ============================================================\n",
    "BASE_DIR = os.path.dirname(os.path.abspath('AI_Predictive_Analytics.ipynb'))\n",
    "CHARTS_DIR = os.path.join(BASE_DIR, 'charts')\n",
    "\n",
    "directories = [\n",
    "    os.path.join(CHARTS_DIR, 'sales_forecasting'),\n",
    "    os.path.join(CHARTS_DIR, 'customer_churn'),\n",
    "    os.path.join(CHARTS_DIR, 'comparison'),\n",
    "    os.path.join(CHARTS_DIR, 'dashboard'),\n",
    "]\n",
    "\n",
    "for d in directories:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "def save_chart(fig, filename, subfolder='', dpi=150):\n",
    "    \"\"\"Save a matplotlib figure to the charts directory.\"\"\"\n",
    "    if subfolder:\n",
    "        path = os.path.join(CHARTS_DIR, subfolder, filename)\n",
    "    else:\n",
    "        path = os.path.join(CHARTS_DIR, filename)\n",
    "    fig.savefig(path, dpi=dpi, bbox_inches='tight', facecolor='white', edgecolor='none')\n",
    "    print(f\"  üíæ Saved: {path}\")\n",
    "\n",
    "def save_plotly_chart(fig, filename, subfolder=''):\n",
    "    \"\"\"Save a plotly figure as PNG.\"\"\"\n",
    "    if subfolder:\n",
    "        path = os.path.join(CHARTS_DIR, subfolder, filename)\n",
    "    else:\n",
    "        path = os.path.join(CHARTS_DIR, filename)\n",
    "    fig.write_image(path, scale=2)\n",
    "    print(f\"  üíæ Saved: {path}\")\n",
    "\n",
    "print(\"‚úÖ Chart directories created:\")\n",
    "for d in directories:\n",
    "    print(f\"   üìÅ {d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253bb131",
   "metadata": {},
   "source": [
    "## 3. Download and Load Datasets\n",
    "\n",
    "We use two real-world datasets:\n",
    "- **Sales Data**: Superstore sales data with product categories, regions, and time-based features\n",
    "- **Churn Data**: Telco Customer Churn dataset with customer demographics and service usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "356ae04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üìä SALES DATASET\n",
      "============================================================\n",
      "Shape: (9800, 18)\n",
      "\n",
      "Columns: ['Row ID', 'Order ID', 'Order Date', 'Ship Date', 'Ship Mode', 'Customer ID', 'Customer Name', 'Segment', 'Country', 'City', 'State', 'Postal Code', 'Region', 'Product ID', 'Category', 'Sub-Category', 'Product Name', 'Sales']\n",
      "\n",
      "Data Types:\n",
      "Row ID             int64\n",
      "Order ID             str\n",
      "Order Date           str\n",
      "Ship Date            str\n",
      "Ship Mode            str\n",
      "Customer ID          str\n",
      "Customer Name        str\n",
      "Segment              str\n",
      "Country              str\n",
      "City                 str\n",
      "State                str\n",
      "Postal Code      float64\n",
      "Region               str\n",
      "Product ID           str\n",
      "Category             str\n",
      "Sub-Category         str\n",
      "Product Name         str\n",
      "Sales            float64\n",
      "dtype: object\n",
      "\n",
      "First 5 rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Row ID</th>\n",
       "      <th>Order ID</th>\n",
       "      <th>Order Date</th>\n",
       "      <th>Ship Date</th>\n",
       "      <th>Ship Mode</th>\n",
       "      <th>Customer ID</th>\n",
       "      <th>Customer Name</th>\n",
       "      <th>Segment</th>\n",
       "      <th>Country</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Postal Code</th>\n",
       "      <th>Region</th>\n",
       "      <th>Product ID</th>\n",
       "      <th>Category</th>\n",
       "      <th>Sub-Category</th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>CA-2017-152156</td>\n",
       "      <td>08/11/2017</td>\n",
       "      <td>11/11/2017</td>\n",
       "      <td>Second Class</td>\n",
       "      <td>CG-12520</td>\n",
       "      <td>Claire Gute</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>United States</td>\n",
       "      <td>Henderson</td>\n",
       "      <td>Kentucky</td>\n",
       "      <td>42420.0000</td>\n",
       "      <td>South</td>\n",
       "      <td>FUR-BO-10001798</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>Bookcases</td>\n",
       "      <td>Bush Somerset Collection Bookcase</td>\n",
       "      <td>261.9600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>CA-2017-152156</td>\n",
       "      <td>08/11/2017</td>\n",
       "      <td>11/11/2017</td>\n",
       "      <td>Second Class</td>\n",
       "      <td>CG-12520</td>\n",
       "      <td>Claire Gute</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>United States</td>\n",
       "      <td>Henderson</td>\n",
       "      <td>Kentucky</td>\n",
       "      <td>42420.0000</td>\n",
       "      <td>South</td>\n",
       "      <td>FUR-CH-10000454</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>Chairs</td>\n",
       "      <td>Hon Deluxe Fabric Upholstered Stacking Chairs,...</td>\n",
       "      <td>731.9400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>CA-2017-138688</td>\n",
       "      <td>12/06/2017</td>\n",
       "      <td>16/06/2017</td>\n",
       "      <td>Second Class</td>\n",
       "      <td>DV-13045</td>\n",
       "      <td>Darrin Van Huff</td>\n",
       "      <td>Corporate</td>\n",
       "      <td>United States</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>California</td>\n",
       "      <td>90036.0000</td>\n",
       "      <td>West</td>\n",
       "      <td>OFF-LA-10000240</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Labels</td>\n",
       "      <td>Self-Adhesive Address Labels for Typewriters b...</td>\n",
       "      <td>14.6200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>US-2016-108966</td>\n",
       "      <td>11/10/2016</td>\n",
       "      <td>18/10/2016</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>SO-20335</td>\n",
       "      <td>Sean O'Donnell</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>United States</td>\n",
       "      <td>Fort Lauderdale</td>\n",
       "      <td>Florida</td>\n",
       "      <td>33311.0000</td>\n",
       "      <td>South</td>\n",
       "      <td>FUR-TA-10000577</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>Tables</td>\n",
       "      <td>Bretford CR4500 Series Slim Rectangular Table</td>\n",
       "      <td>957.5775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>US-2016-108966</td>\n",
       "      <td>11/10/2016</td>\n",
       "      <td>18/10/2016</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>SO-20335</td>\n",
       "      <td>Sean O'Donnell</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>United States</td>\n",
       "      <td>Fort Lauderdale</td>\n",
       "      <td>Florida</td>\n",
       "      <td>33311.0000</td>\n",
       "      <td>South</td>\n",
       "      <td>OFF-ST-10000760</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Storage</td>\n",
       "      <td>Eldon Fold 'N Roll Cart System</td>\n",
       "      <td>22.3680</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Row ID        Order ID  Order Date   Ship Date       Ship Mode Customer ID  \\\n",
       "0       1  CA-2017-152156  08/11/2017  11/11/2017    Second Class    CG-12520   \n",
       "1       2  CA-2017-152156  08/11/2017  11/11/2017    Second Class    CG-12520   \n",
       "2       3  CA-2017-138688  12/06/2017  16/06/2017    Second Class    DV-13045   \n",
       "3       4  US-2016-108966  11/10/2016  18/10/2016  Standard Class    SO-20335   \n",
       "4       5  US-2016-108966  11/10/2016  18/10/2016  Standard Class    SO-20335   \n",
       "\n",
       "     Customer Name    Segment        Country             City       State  \\\n",
       "0      Claire Gute   Consumer  United States        Henderson    Kentucky   \n",
       "1      Claire Gute   Consumer  United States        Henderson    Kentucky   \n",
       "2  Darrin Van Huff  Corporate  United States      Los Angeles  California   \n",
       "3   Sean O'Donnell   Consumer  United States  Fort Lauderdale     Florida   \n",
       "4   Sean O'Donnell   Consumer  United States  Fort Lauderdale     Florida   \n",
       "\n",
       "   Postal Code Region       Product ID         Category Sub-Category  \\\n",
       "0   42420.0000  South  FUR-BO-10001798        Furniture    Bookcases   \n",
       "1   42420.0000  South  FUR-CH-10000454        Furniture       Chairs   \n",
       "2   90036.0000   West  OFF-LA-10000240  Office Supplies       Labels   \n",
       "3   33311.0000  South  FUR-TA-10000577        Furniture       Tables   \n",
       "4   33311.0000  South  OFF-ST-10000760  Office Supplies      Storage   \n",
       "\n",
       "                                        Product Name    Sales  \n",
       "0                  Bush Somerset Collection Bookcase 261.9600  \n",
       "1  Hon Deluxe Fabric Upholstered Stacking Chairs,... 731.9400  \n",
       "2  Self-Adhesive Address Labels for Typewriters b...  14.6200  \n",
       "3      Bretford CR4500 Series Slim Rectangular Table 957.5775  \n",
       "4                     Eldon Fold 'N Roll Cart System  22.3680  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Load Datasets\n",
    "# ============================================================\n",
    "\n",
    "# Load Sales Data\n",
    "sales_df = pd.read_csv('sales_data.csv', encoding='latin-1')\n",
    "print(\"=\" * 60)\n",
    "print(\"üìä SALES DATASET\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Shape: {sales_df.shape}\")\n",
    "print(f\"\\nColumns: {list(sales_df.columns)}\")\n",
    "print(f\"\\nData Types:\\n{sales_df.dtypes}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "sales_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ae30dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üë• CUSTOMER CHURN DATASET\n",
      "============================================================\n",
      "Shape: (7043, 21)\n",
      "\n",
      "Columns: ['customerID', 'gender', 'SeniorCitizen', 'Partner', 'Dependents', 'tenure', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod', 'MonthlyCharges', 'TotalCharges', 'Churn']\n",
      "\n",
      "Data Types:\n",
      "customerID              str\n",
      "gender                  str\n",
      "SeniorCitizen         int64\n",
      "Partner                 str\n",
      "Dependents              str\n",
      "tenure                int64\n",
      "PhoneService            str\n",
      "MultipleLines           str\n",
      "InternetService         str\n",
      "OnlineSecurity          str\n",
      "OnlineBackup            str\n",
      "DeviceProtection        str\n",
      "TechSupport             str\n",
      "StreamingTV             str\n",
      "StreamingMovies         str\n",
      "Contract                str\n",
      "PaperlessBilling        str\n",
      "PaymentMethod           str\n",
      "MonthlyCharges      float64\n",
      "TotalCharges            str\n",
      "Churn                   str\n",
      "dtype: object\n",
      "\n",
      "First 5 rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customerID</th>\n",
       "      <th>gender</th>\n",
       "      <th>SeniorCitizen</th>\n",
       "      <th>Partner</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>tenure</th>\n",
       "      <th>PhoneService</th>\n",
       "      <th>MultipleLines</th>\n",
       "      <th>InternetService</th>\n",
       "      <th>OnlineSecurity</th>\n",
       "      <th>OnlineBackup</th>\n",
       "      <th>DeviceProtection</th>\n",
       "      <th>TechSupport</th>\n",
       "      <th>StreamingTV</th>\n",
       "      <th>StreamingMovies</th>\n",
       "      <th>Contract</th>\n",
       "      <th>PaperlessBilling</th>\n",
       "      <th>PaymentMethod</th>\n",
       "      <th>MonthlyCharges</th>\n",
       "      <th>TotalCharges</th>\n",
       "      <th>Churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7590-VHVEG</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>No phone service</td>\n",
       "      <td>DSL</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Electronic check</td>\n",
       "      <td>29.8500</td>\n",
       "      <td>29.85</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5575-GNVDE</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>34</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>DSL</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>One year</td>\n",
       "      <td>No</td>\n",
       "      <td>Mailed check</td>\n",
       "      <td>56.9500</td>\n",
       "      <td>1889.5</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3668-QPYBK</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>2</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>DSL</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Mailed check</td>\n",
       "      <td>53.8500</td>\n",
       "      <td>108.15</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7795-CFOCW</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>45</td>\n",
       "      <td>No</td>\n",
       "      <td>No phone service</td>\n",
       "      <td>DSL</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>One year</td>\n",
       "      <td>No</td>\n",
       "      <td>Bank transfer (automatic)</td>\n",
       "      <td>42.3000</td>\n",
       "      <td>1840.75</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9237-HQITU</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>2</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Fiber optic</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Electronic check</td>\n",
       "      <td>70.7000</td>\n",
       "      <td>151.65</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customerID  gender  SeniorCitizen Partner Dependents  tenure PhoneService  \\\n",
       "0  7590-VHVEG  Female              0     Yes         No       1           No   \n",
       "1  5575-GNVDE    Male              0      No         No      34          Yes   \n",
       "2  3668-QPYBK    Male              0      No         No       2          Yes   \n",
       "3  7795-CFOCW    Male              0      No         No      45           No   \n",
       "4  9237-HQITU  Female              0      No         No       2          Yes   \n",
       "\n",
       "      MultipleLines InternetService OnlineSecurity OnlineBackup  \\\n",
       "0  No phone service             DSL             No          Yes   \n",
       "1                No             DSL            Yes           No   \n",
       "2                No             DSL            Yes          Yes   \n",
       "3  No phone service             DSL            Yes           No   \n",
       "4                No     Fiber optic             No           No   \n",
       "\n",
       "  DeviceProtection TechSupport StreamingTV StreamingMovies        Contract  \\\n",
       "0               No          No          No              No  Month-to-month   \n",
       "1              Yes          No          No              No        One year   \n",
       "2               No          No          No              No  Month-to-month   \n",
       "3              Yes         Yes          No              No        One year   \n",
       "4               No          No          No              No  Month-to-month   \n",
       "\n",
       "  PaperlessBilling              PaymentMethod  MonthlyCharges TotalCharges  \\\n",
       "0              Yes           Electronic check         29.8500        29.85   \n",
       "1               No               Mailed check         56.9500       1889.5   \n",
       "2              Yes               Mailed check         53.8500       108.15   \n",
       "3               No  Bank transfer (automatic)         42.3000      1840.75   \n",
       "4              Yes           Electronic check         70.7000       151.65   \n",
       "\n",
       "  Churn  \n",
       "0    No  \n",
       "1    No  \n",
       "2   Yes  \n",
       "3    No  \n",
       "4   Yes  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Churn Data\n",
    "churn_df = pd.read_csv('churn_data.csv')\n",
    "print(\"=\" * 60)\n",
    "print(\"üë• CUSTOMER CHURN DATASET\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Shape: {churn_df.shape}\")\n",
    "print(f\"\\nColumns: {list(churn_df.columns)}\")\n",
    "print(f\"\\nData Types:\\n{churn_df.dtypes}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "churn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0b1b6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üìä SALES DATA - Statistical Summary\n",
      "============================================================\n",
      "         Row ID  Postal Code      Sales\n",
      "count 9800.0000    9789.0000  9800.0000\n",
      "mean  4900.5000   55273.3224   230.7691\n",
      "std   2829.1607   32041.2234   626.6519\n",
      "min      1.0000    1040.0000     0.4440\n",
      "25%   2450.7500   23223.0000    17.2480\n",
      "50%   4900.5000   58103.0000    54.4900\n",
      "75%   7350.2500   90008.0000   210.6050\n",
      "max   9800.0000   99301.0000 22638.4800\n",
      "\n",
      "Missing Values:\n",
      "Row ID            0\n",
      "Order ID          0\n",
      "Order Date        0\n",
      "Ship Date         0\n",
      "Ship Mode         0\n",
      "Customer ID       0\n",
      "Customer Name     0\n",
      "Segment           0\n",
      "Country           0\n",
      "City              0\n",
      "State             0\n",
      "Postal Code      11\n",
      "Region            0\n",
      "Product ID        0\n",
      "Category          0\n",
      "Sub-Category      0\n",
      "Product Name      0\n",
      "Sales             0\n",
      "dtype: int64\n",
      "\n",
      "Total Missing: 11\n",
      "\n",
      "============================================================\n",
      "üë• CHURN DATA - Statistical Summary\n",
      "============================================================\n",
      "       SeniorCitizen    tenure  MonthlyCharges\n",
      "count      7043.0000 7043.0000       7043.0000\n",
      "mean          0.1621   32.3711         64.7617\n",
      "std           0.3686   24.5595         30.0900\n",
      "min           0.0000    0.0000         18.2500\n",
      "25%           0.0000    9.0000         35.5000\n",
      "50%           0.0000   29.0000         70.3500\n",
      "75%           0.0000   55.0000         89.8500\n",
      "max           1.0000   72.0000        118.7500\n",
      "\n",
      "Missing Values:\n",
      "customerID          0\n",
      "gender              0\n",
      "SeniorCitizen       0\n",
      "Partner             0\n",
      "Dependents          0\n",
      "tenure              0\n",
      "PhoneService        0\n",
      "MultipleLines       0\n",
      "InternetService     0\n",
      "OnlineSecurity      0\n",
      "OnlineBackup        0\n",
      "DeviceProtection    0\n",
      "TechSupport         0\n",
      "StreamingTV         0\n",
      "StreamingMovies     0\n",
      "Contract            0\n",
      "PaperlessBilling    0\n",
      "PaymentMethod       0\n",
      "MonthlyCharges      0\n",
      "TotalCharges        0\n",
      "Churn               0\n",
      "dtype: int64\n",
      "\n",
      "Total Missing: 0\n"
     ]
    }
   ],
   "source": [
    "# Dataset Summary Statistics\n",
    "print(\"=\" * 60)\n",
    "print(\"üìä SALES DATA - Statistical Summary\")\n",
    "print(\"=\" * 60)\n",
    "print(sales_df.describe())\n",
    "print(f\"\\nMissing Values:\\n{sales_df.isnull().sum()}\")\n",
    "print(f\"\\nTotal Missing: {sales_df.isnull().sum().sum()}\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üë• CHURN DATA - Statistical Summary\")\n",
    "print(\"=\" * 60)\n",
    "print(churn_df.describe())\n",
    "print(f\"\\nMissing Values:\\n{churn_df.isnull().sum()}\")\n",
    "print(f\"\\nTotal Missing: {churn_df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc8bd1d",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 1: SALES FORECASTING MODEL\n",
    "---\n",
    "## 4. Exploratory Data Analysis - Sales Data\n",
    "\n",
    "Comprehensive visual exploration of the sales dataset to understand patterns, distributions, and relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01369e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Date features extracted\n",
      "Date range: 2015-01-03 00:00:00 to 2018-12-30 00:00:00\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Order Date</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Quarter</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>IsWeekend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-11-08</td>\n",
       "      <td>2017</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-11-08</td>\n",
       "      <td>2017</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-06-12</td>\n",
       "      <td>2017</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-10-11</td>\n",
       "      <td>2016</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-10-11</td>\n",
       "      <td>2016</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2015-06-09</td>\n",
       "      <td>2015</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2015-06-09</td>\n",
       "      <td>2015</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2015-06-09</td>\n",
       "      <td>2015</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2015-06-09</td>\n",
       "      <td>2015</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2015-06-09</td>\n",
       "      <td>2015</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Order Date  Year  Month  Quarter  DayOfWeek  IsWeekend\n",
       "0 2017-11-08  2017     11        4          2          0\n",
       "1 2017-11-08  2017     11        4          2          0\n",
       "2 2017-06-12  2017      6        2          0          0\n",
       "3 2016-10-11  2016     10        4          1          0\n",
       "4 2016-10-11  2016     10        4          1          0\n",
       "5 2015-06-09  2015      6        2          1          0\n",
       "6 2015-06-09  2015      6        2          1          0\n",
       "7 2015-06-09  2015      6        2          1          0\n",
       "8 2015-06-09  2015      6        2          1          0\n",
       "9 2015-06-09  2015      6        2          1          0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 4.1 Sales Data Preparation for EDA\n",
    "# ============================================================\n",
    "sales_df['Order Date'] = pd.to_datetime(sales_df['Order Date'], format='%d/%m/%Y', dayfirst=True, errors='coerce')\n",
    "# Try alternative format if parsing failed\n",
    "if sales_df['Order Date'].isnull().sum() > len(sales_df) * 0.5:\n",
    "    sales_df['Order Date'] = pd.to_datetime(sales_df['Order Date'], format='mixed', dayfirst=False)\n",
    "\n",
    "sales_df['Ship Date'] = pd.to_datetime(sales_df['Ship Date'], format='%d/%m/%Y', dayfirst=True, errors='coerce')\n",
    "if sales_df['Ship Date'].isnull().sum() > len(sales_df) * 0.5:\n",
    "    sales_df['Ship Date'] = pd.to_datetime(sales_df['Ship Date'], format='mixed', dayfirst=False)\n",
    "\n",
    "sales_df['Year'] = sales_df['Order Date'].dt.year\n",
    "sales_df['Month'] = sales_df['Order Date'].dt.month\n",
    "sales_df['Day'] = sales_df['Order Date'].dt.day\n",
    "sales_df['DayOfWeek'] = sales_df['Order Date'].dt.dayofweek\n",
    "sales_df['Quarter'] = sales_df['Order Date'].dt.quarter\n",
    "sales_df['WeekOfYear'] = sales_df['Order Date'].dt.isocalendar().week.astype(int)\n",
    "sales_df['IsWeekend'] = (sales_df['DayOfWeek'] >= 5).astype(int)\n",
    "sales_df['Month_Name'] = sales_df['Order Date'].dt.month_name()\n",
    "\n",
    "print(\"‚úÖ Date features extracted\")\n",
    "print(f\"Date range: {sales_df['Order Date'].min()} to {sales_df['Order Date'].max()}\")\n",
    "sales_df[['Order Date', 'Year', 'Month', 'Quarter', 'DayOfWeek', 'IsWeekend']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "767a6a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  üíæ Saved: /Users/usama/Desktop/dummy/charts/sales_forecasting/01_sales_distribution.png\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 4.2 Sales Distribution & Missing Values\n",
    "# ============================================================\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "# Sales Distribution\n",
    "axes[0].hist(sales_df['Sales'], bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "axes[0].set_title('Sales Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Sales ($)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].axvline(sales_df['Sales'].mean(), color='red', linestyle='--', label=f\"Mean: ${sales_df['Sales'].mean():.2f}\")\n",
    "axes[0].axvline(sales_df['Sales'].median(), color='green', linestyle='--', label=f\"Median: ${sales_df['Sales'].median():.2f}\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Log-transformed Sales Distribution\n",
    "axes[1].hist(np.log1p(sales_df['Sales']), bins=50, color='coral', edgecolor='black', alpha=0.7)\n",
    "axes[1].set_title('Log-Transformed Sales Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Log(Sales)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "\n",
    "# Missing Values Heatmap\n",
    "missing = sales_df.isnull().sum()\n",
    "missing = missing[missing > 0]\n",
    "if len(missing) > 0:\n",
    "    axes[2].barh(missing.index, missing.values, color='tomato')\n",
    "    axes[2].set_title('Missing Values Count', fontsize=14, fontweight='bold')\n",
    "else:\n",
    "    axes[2].text(0.5, 0.5, 'No Missing Values!', ha='center', va='center', fontsize=16, color='green', transform=axes[2].transAxes)\n",
    "    axes[2].set_title('Missing Values Check', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "save_chart(fig, '01_sales_distribution.png', 'sales_forecasting')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e849d9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  üíæ Saved: /Users/usama/Desktop/dummy/charts/sales_forecasting/02_sales_time_series.png\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 4.3 Sales Over Time - Time Series Plot\n",
    "# ============================================================\n",
    "daily_sales = sales_df.groupby('Order Date')['Sales'].sum().reset_index()\n",
    "daily_sales = daily_sales.sort_values('Order Date')\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(18, 10))\n",
    "\n",
    "# Daily Sales\n",
    "axes[0].plot(daily_sales['Order Date'], daily_sales['Sales'], color='steelblue', alpha=0.6, linewidth=0.8)\n",
    "# Add 30-day rolling average\n",
    "rolling_avg = daily_sales['Sales'].rolling(window=30, min_periods=1).mean()\n",
    "axes[0].plot(daily_sales['Order Date'], rolling_avg, color='red', linewidth=2, label='30-Day Rolling Average')\n",
    "axes[0].set_title('Daily Sales Over Time with Rolling Average', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Date')\n",
    "axes[0].set_ylabel('Sales ($)')\n",
    "axes[0].legend(fontsize=12)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Monthly Sales\n",
    "monthly_sales = sales_df.groupby([sales_df['Order Date'].dt.to_period('M')])['Sales'].sum().reset_index()\n",
    "monthly_sales['Order Date'] = monthly_sales['Order Date'].dt.to_timestamp()\n",
    "axes[1].plot(monthly_sales['Order Date'], monthly_sales['Sales'], marker='o', color='darkgreen', linewidth=2, markersize=5)\n",
    "axes[1].fill_between(monthly_sales['Order Date'], monthly_sales['Sales'], alpha=0.2, color='green')\n",
    "axes[1].set_title('Monthly Sales Trend', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Date')\n",
    "axes[1].set_ylabel('Total Sales ($)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "save_chart(fig, '02_sales_time_series.png', 'sales_forecasting')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c46a18d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  üíæ Saved: /Users/usama/Desktop/dummy/charts/sales_forecasting/03_sales_category_analysis.png\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 4.4 Seasonal & Category Analysis\n",
    "# ============================================================\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "\n",
    "# Monthly Seasonal Pattern\n",
    "month_order = ['January','February','March','April','May','June','July','August','September','October','November','December']\n",
    "monthly_pattern = sales_df.groupby('Month_Name')['Sales'].mean().reindex(month_order)\n",
    "colors = plt.cm.RdYlGn(np.linspace(0.2, 0.8, 12))\n",
    "axes[0,0].bar(range(12), monthly_pattern.values, color=colors, edgecolor='black', alpha=0.8)\n",
    "axes[0,0].set_xticks(range(12))\n",
    "axes[0,0].set_xticklabels([m[:3] for m in month_order], rotation=45)\n",
    "axes[0,0].set_title('Average Sales by Month (Seasonal Pattern)', fontsize=13, fontweight='bold')\n",
    "axes[0,0].set_ylabel('Average Sales ($)')\n",
    "\n",
    "# Sales by Category\n",
    "cat_sales = sales_df.groupby('Category')['Sales'].agg(['sum', 'mean', 'count']).sort_values('sum', ascending=True)\n",
    "axes[0,1].barh(cat_sales.index, cat_sales['sum'], color=['#FF6B6B', '#4ECDC4', '#45B7D1'], edgecolor='black')\n",
    "axes[0,1].set_title('Total Sales by Category', fontsize=13, fontweight='bold')\n",
    "axes[0,1].set_xlabel('Total Sales ($)')\n",
    "for i, v in enumerate(cat_sales['sum']):\n",
    "    axes[0,1].text(v + 1000, i, f'${v:,.0f}', va='center', fontweight='bold')\n",
    "\n",
    "# Sales by Region\n",
    "region_sales = sales_df.groupby('Region')['Sales'].sum().sort_values(ascending=True)\n",
    "axes[1,0].barh(region_sales.index, region_sales.values, color=['#FFB347', '#87CEEB', '#98D8C8', '#F7DC6F'], edgecolor='black')\n",
    "axes[1,0].set_title('Total Sales by Region', fontsize=13, fontweight='bold')\n",
    "axes[1,0].set_xlabel('Total Sales ($)')\n",
    "for i, v in enumerate(region_sales.values):\n",
    "    axes[1,0].text(v + 1000, i, f'${v:,.0f}', va='center', fontweight='bold')\n",
    "\n",
    "# Sales by Segment\n",
    "seg_sales = sales_df.groupby('Segment')['Sales'].sum().sort_values(ascending=True)\n",
    "axes[1,1].barh(seg_sales.index, seg_sales.values, color=['#DDA0DD', '#90EE90', '#F0E68C'], edgecolor='black')\n",
    "axes[1,1].set_title('Total Sales by Segment', fontsize=13, fontweight='bold')\n",
    "axes[1,1].set_xlabel('Total Sales ($)')\n",
    "for i, v in enumerate(seg_sales.values):\n",
    "    axes[1,1].text(v + 1000, i, f'${v:,.0f}', va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "save_chart(fig, '03_sales_category_analysis.png', 'sales_forecasting')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b271be78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  üíæ Saved: /Users/usama/Desktop/dummy/charts/sales_forecasting/04_sales_subcategory_boxplot.png\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 4.5 Sub-Category Analysis & Box Plots\n",
    "# ============================================================\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 14))\n",
    "\n",
    "# Top 10 Sub-Categories by Sales\n",
    "subcat_sales = sales_df.groupby('Sub-Category')['Sales'].sum().sort_values(ascending=True)\n",
    "axes[0,0].barh(subcat_sales.index, subcat_sales.values, color=plt.cm.viridis(np.linspace(0.2, 0.9, len(subcat_sales))), edgecolor='black')\n",
    "axes[0,0].set_title('Sales by Sub-Category', fontsize=13, fontweight='bold')\n",
    "axes[0,0].set_xlabel('Total Sales ($)')\n",
    "\n",
    "# Box Plot - Sales by Category\n",
    "sales_df.boxplot(column='Sales', by='Category', ax=axes[0,1], patch_artist=True,\n",
    "                 boxprops=dict(facecolor='lightblue'), medianprops=dict(color='red', linewidth=2))\n",
    "axes[0,1].set_title('Sales Distribution by Category', fontsize=13, fontweight='bold')\n",
    "axes[0,1].set_xlabel('Category')\n",
    "axes[0,1].set_ylabel('Sales ($)')\n",
    "plt.sca(axes[0,1])\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Quarterly Sales Trend\n",
    "quarterly_sales = sales_df.groupby(['Year', 'Quarter'])['Sales'].sum().reset_index()\n",
    "for year in quarterly_sales['Year'].unique():\n",
    "    year_data = quarterly_sales[quarterly_sales['Year'] == year]\n",
    "    axes[1,0].plot(year_data['Quarter'], year_data['Sales'], marker='o', linewidth=2, label=str(int(year)))\n",
    "axes[1,0].set_title('Quarterly Sales by Year', fontsize=13, fontweight='bold')\n",
    "axes[1,0].set_xlabel('Quarter')\n",
    "axes[1,0].set_ylabel('Total Sales ($)')\n",
    "axes[1,0].legend(title='Year')\n",
    "axes[1,0].set_xticks([1, 2, 3, 4])\n",
    "\n",
    "# Ship Mode Distribution\n",
    "ship_counts = sales_df['Ship Mode'].value_counts()\n",
    "axes[1,1].pie(ship_counts.values, labels=ship_counts.index, autopct='%1.1f%%', \n",
    "              colors=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'], startangle=90,\n",
    "              explode=[0.05]*len(ship_counts))\n",
    "axes[1,1].set_title('Ship Mode Distribution', fontsize=13, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "save_chart(fig, '04_sales_subcategory_boxplot.png', 'sales_forecasting')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63d005ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  üíæ Saved: /Users/usama/Desktop/dummy/charts/sales_forecasting/05_sales_correlation_dayofweek.png\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 4.6 Correlation Heatmap & Day-of-Week Analysis\n",
    "# ============================================================\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "# Correlation Heatmap for numerical features\n",
    "num_cols = sales_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "corr_matrix = sales_df[num_cols].corr()\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', cmap='RdBu_r', center=0,\n",
    "            ax=axes[0], square=True, linewidths=0.5, cbar_kws={'shrink': 0.8})\n",
    "axes[0].set_title('Correlation Heatmap - Sales Features', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Sales by Day of Week\n",
    "day_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "dow_sales = sales_df.groupby('DayOfWeek')['Sales'].mean()\n",
    "colors = ['#FF6B6B' if i >= 5 else '#4ECDC4' for i in range(7)]\n",
    "axes[1].bar(range(7), dow_sales.values, color=colors, edgecolor='black', alpha=0.8)\n",
    "axes[1].set_xticks(range(7))\n",
    "axes[1].set_xticklabels([d[:3] for d in day_names], rotation=45)\n",
    "axes[1].set_title('Average Sales by Day of Week', fontsize=13, fontweight='bold')\n",
    "axes[1].set_ylabel('Average Sales ($)')\n",
    "axes[1].legend(['Weekend', 'Weekday'], loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "save_chart(fig, '05_sales_correlation_dayofweek.png', 'sales_forecasting')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0ae271",
   "metadata": {},
   "source": [
    "## 5. Exploratory Data Analysis - Customer Churn Data\n",
    "\n",
    "Comprehensive visual exploration of the Telco Customer Churn dataset to understand churn patterns, demographics, and service usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a8da513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  üíæ Saved: /Users/usama/Desktop/dummy/charts/customer_churn/01_churn_distribution_demographics.png\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 5.1 Churn Distribution & Demographics\n",
    "# ============================================================\n",
    "# Convert TotalCharges to numeric\n",
    "churn_df['TotalCharges'] = pd.to_numeric(churn_df['TotalCharges'], errors='coerce')\n",
    "churn_df['TotalCharges'].fillna(churn_df['TotalCharges'].median(), inplace=True)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Churn Distribution - Pie Chart\n",
    "churn_counts = churn_df['Churn'].value_counts()\n",
    "axes[0,0].pie(churn_counts.values, labels=['No Churn', 'Churn'], autopct='%1.1f%%',\n",
    "              colors=['#4ECDC4', '#FF6B6B'], startangle=90, explode=[0, 0.1],\n",
    "              textprops={'fontsize': 14, 'fontweight': 'bold'})\n",
    "axes[0,0].set_title('Customer Churn Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Churn Distribution - Count Plot\n",
    "sns.countplot(data=churn_df, x='Churn', ax=axes[0,1], palette=['#4ECDC4', '#FF6B6B'], edgecolor='black')\n",
    "axes[0,1].set_title('Churn Count', fontsize=14, fontweight='bold')\n",
    "axes[0,1].set_ylabel('Count')\n",
    "for p in axes[0,1].patches:\n",
    "    axes[0,1].annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width()/2., p.get_height()),\n",
    "                       ha='center', va='bottom', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Gender vs Churn\n",
    "ct_gender = pd.crosstab(churn_df['gender'], churn_df['Churn'], normalize='index') * 100\n",
    "ct_gender.plot(kind='bar', ax=axes[1,0], color=['#4ECDC4', '#FF6B6B'], edgecolor='black', alpha=0.8)\n",
    "axes[1,0].set_title('Churn Rate by Gender', fontsize=14, fontweight='bold')\n",
    "axes[1,0].set_ylabel('Percentage (%)')\n",
    "axes[1,0].set_xticklabels(axes[1,0].get_xticklabels(), rotation=0)\n",
    "axes[1,0].legend(['No Churn', 'Churn'])\n",
    "\n",
    "# Senior Citizen vs Churn\n",
    "ct_senior = pd.crosstab(churn_df['SeniorCitizen'].map({0: 'Non-Senior', 1: 'Senior'}), \n",
    "                         churn_df['Churn'], normalize='index') * 100\n",
    "ct_senior.plot(kind='bar', ax=axes[1,1], color=['#4ECDC4', '#FF6B6B'], edgecolor='black', alpha=0.8)\n",
    "axes[1,1].set_title('Churn Rate by Senior Citizen Status', fontsize=14, fontweight='bold')\n",
    "axes[1,1].set_ylabel('Percentage (%)')\n",
    "axes[1,1].set_xticklabels(axes[1,1].get_xticklabels(), rotation=0)\n",
    "axes[1,1].legend(['No Churn', 'Churn'])\n",
    "\n",
    "plt.tight_layout()\n",
    "save_chart(fig, '01_churn_distribution_demographics.png', 'customer_churn')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b1fa1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  üíæ Saved: /Users/usama/Desktop/dummy/charts/customer_churn/02_churn_service_contract.png\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 5.2 Service Usage & Contract Analysis\n",
    "# ============================================================\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "\n",
    "# Internet Service vs Churn\n",
    "ct_internet = pd.crosstab(churn_df['InternetService'], churn_df['Churn'], normalize='index') * 100\n",
    "ct_internet.plot(kind='bar', ax=axes[0,0], color=['#4ECDC4', '#FF6B6B'], edgecolor='black', alpha=0.8)\n",
    "axes[0,0].set_title('Churn Rate by Internet Service', fontsize=13, fontweight='bold')\n",
    "axes[0,0].set_ylabel('Percentage (%)')\n",
    "axes[0,0].set_xticklabels(axes[0,0].get_xticklabels(), rotation=0)\n",
    "axes[0,0].legend(['No Churn', 'Churn'])\n",
    "\n",
    "# Contract Type vs Churn\n",
    "ct_contract = pd.crosstab(churn_df['Contract'], churn_df['Churn'], normalize='index') * 100\n",
    "ct_contract.plot(kind='bar', ax=axes[0,1], color=['#4ECDC4', '#FF6B6B'], edgecolor='black', alpha=0.8)\n",
    "axes[0,1].set_title('Churn Rate by Contract Type', fontsize=13, fontweight='bold')\n",
    "axes[0,1].set_ylabel('Percentage (%)')\n",
    "axes[0,1].set_xticklabels(axes[0,1].get_xticklabels(), rotation=0)\n",
    "axes[0,1].legend(['No Churn', 'Churn'])\n",
    "\n",
    "# Payment Method vs Churn\n",
    "ct_payment = pd.crosstab(churn_df['PaymentMethod'], churn_df['Churn'], normalize='index') * 100\n",
    "ct_payment.plot(kind='bar', ax=axes[1,0], color=['#4ECDC4', '#FF6B6B'], edgecolor='black', alpha=0.8)\n",
    "axes[1,0].set_title('Churn Rate by Payment Method', fontsize=13, fontweight='bold')\n",
    "axes[1,0].set_ylabel('Percentage (%)')\n",
    "axes[1,0].set_xticklabels(axes[1,0].get_xticklabels(), rotation=30, ha='right')\n",
    "axes[1,0].legend(['No Churn', 'Churn'])\n",
    "\n",
    "# Partner & Dependents vs Churn\n",
    "services = ['Partner', 'Dependents', 'PhoneService', 'PaperlessBilling']\n",
    "churn_rates = []\n",
    "for s in services:\n",
    "    rate = churn_df[churn_df[s] == 'Yes']['Churn'].value_counts(normalize=True).get('Yes', 0) * 100\n",
    "    churn_rates.append(rate)\n",
    "axes[1,1].barh(services, churn_rates, color=['#FFB347', '#87CEEB', '#98D8C8', '#DDA0DD'], edgecolor='black')\n",
    "axes[1,1].set_title('Churn Rate by Service (Yes Only)', fontsize=13, fontweight='bold')\n",
    "axes[1,1].set_xlabel('Churn Rate (%)')\n",
    "for i, v in enumerate(churn_rates):\n",
    "    axes[1,1].text(v + 0.5, i, f'{v:.1f}%', va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "save_chart(fig, '02_churn_service_contract.png', 'customer_churn')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9090f183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  üíæ Saved: /Users/usama/Desktop/dummy/charts/customer_churn/03_churn_numerical_distributions.png\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 5.3 Numerical Features Distribution & Correlation\n",
    "# ============================================================\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "\n",
    "# Tenure Distribution by Churn\n",
    "for label, color in [('No', '#4ECDC4'), ('Yes', '#FF6B6B')]:\n",
    "    subset = churn_df[churn_df['Churn'] == label]\n",
    "    axes[0,0].hist(subset['tenure'], bins=30, alpha=0.6, color=color, label=f'Churn={label}', edgecolor='black')\n",
    "axes[0,0].set_title('Tenure Distribution by Churn', fontsize=13, fontweight='bold')\n",
    "axes[0,0].set_xlabel('Tenure (months)')\n",
    "axes[0,0].legend()\n",
    "\n",
    "# Monthly Charges Distribution by Churn\n",
    "for label, color in [('No', '#4ECDC4'), ('Yes', '#FF6B6B')]:\n",
    "    subset = churn_df[churn_df['Churn'] == label]\n",
    "    axes[0,1].hist(subset['MonthlyCharges'], bins=30, alpha=0.6, color=color, label=f'Churn={label}', edgecolor='black')\n",
    "axes[0,1].set_title('Monthly Charges by Churn', fontsize=13, fontweight='bold')\n",
    "axes[0,1].set_xlabel('Monthly Charges ($)')\n",
    "axes[0,1].legend()\n",
    "\n",
    "# Total Charges Distribution by Churn\n",
    "for label, color in [('No', '#4ECDC4'), ('Yes', '#FF6B6B')]:\n",
    "    subset = churn_df[churn_df['Churn'] == label]\n",
    "    axes[0,2].hist(subset['TotalCharges'], bins=30, alpha=0.6, color=color, label=f'Churn={label}', edgecolor='black')\n",
    "axes[0,2].set_title('Total Charges by Churn', fontsize=13, fontweight='bold')\n",
    "axes[0,2].set_xlabel('Total Charges ($)')\n",
    "axes[0,2].legend()\n",
    "\n",
    "# Box Plots - Tenure, MonthlyCharges, TotalCharges by Churn\n",
    "for idx, col in enumerate(['tenure', 'MonthlyCharges', 'TotalCharges']):\n",
    "    churn_df.boxplot(column=col, by='Churn', ax=axes[1, idx], patch_artist=True,\n",
    "                     boxprops=dict(facecolor='lightblue'), medianprops=dict(color='red', linewidth=2))\n",
    "    axes[1, idx].set_title(f'{col} by Churn Status', fontsize=13, fontweight='bold')\n",
    "    axes[1, idx].set_xlabel('Churn')\n",
    "\n",
    "plt.tight_layout()\n",
    "save_chart(fig, '03_churn_numerical_distributions.png', 'customer_churn')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2940c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  üíæ Saved: /Users/usama/Desktop/dummy/charts/customer_churn/04_churn_correlation_services.png\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 5.4 Churn Correlation Heatmap & Stacked Bars\n",
    "# ============================================================\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Encode churn for correlation\n",
    "churn_encoded = churn_df.copy()\n",
    "churn_encoded['Churn_Binary'] = (churn_encoded['Churn'] == 'Yes').astype(int)\n",
    "churn_num = churn_encoded.select_dtypes(include=[np.number])\n",
    "corr = churn_num.corr()\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "sns.heatmap(corr, mask=mask, annot=True, fmt='.2f', cmap='RdBu_r', center=0,\n",
    "            ax=axes[0], square=True, linewidths=0.5, cbar_kws={'shrink': 0.8})\n",
    "axes[0].set_title('Correlation Heatmap - Churn Features', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Stacked Bar - Multiple Services vs Churn\n",
    "services_cat = ['OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies']\n",
    "churn_rates_services = []\n",
    "for s in services_cat:\n",
    "    if s in churn_df.columns:\n",
    "        rate = churn_df[churn_df[s] == 'Yes']['Churn'].value_counts(normalize=True).get('Yes', 0) * 100\n",
    "        no_rate = churn_df[churn_df[s] == 'No']['Churn'].value_counts(normalize=True).get('Yes', 0) * 100\n",
    "        churn_rates_services.append({'Service': s, 'With Service': rate, 'Without Service': no_rate})\n",
    "\n",
    "svc_df = pd.DataFrame(churn_rates_services)\n",
    "x_pos = range(len(svc_df))\n",
    "width = 0.35\n",
    "axes[1].bar([p - width/2 for p in x_pos], svc_df['With Service'], width, label='With Service', color='#4ECDC4', edgecolor='black')\n",
    "axes[1].bar([p + width/2 for p in x_pos], svc_df['Without Service'], width, label='Without Service', color='#FF6B6B', edgecolor='black')\n",
    "axes[1].set_xticks(x_pos)\n",
    "axes[1].set_xticklabels(svc_df['Service'], rotation=45, ha='right')\n",
    "axes[1].set_title('Churn Rate: With vs Without Service', fontsize=13, fontweight='bold')\n",
    "axes[1].set_ylabel('Churn Rate (%)')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "save_chart(fig, '04_churn_correlation_services.png', 'customer_churn')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ad0a22",
   "metadata": {},
   "source": [
    "## 6. Data Preprocessing & Feature Engineering for Sales Forecasting\n",
    "\n",
    "Prepare the sales data for ML models by aggregating daily sales, creating lag features, rolling statistics, and cyclical encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c87cd71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Feature engineering complete!\n",
      "Dataset shape: (1428, 36)\n",
      "Features: ['Date', 'Sales', 'Quantity', 'Year', 'Month', 'Day', 'DayOfWeek', 'Quarter', 'WeekOfYear', 'IsWeekend', 'DayOfYear', 'Month_sin', 'Month_cos', 'DayOfWeek_sin', 'DayOfWeek_cos', 'Sales_lag_1', 'Sales_lag_3', 'Sales_lag_7', 'Sales_lag_14', 'Sales_lag_21', 'Sales_lag_30', 'Sales_rolling_mean_7', 'Sales_rolling_std_7', 'Sales_rolling_max_7', 'Sales_rolling_min_7', 'Sales_rolling_mean_14', 'Sales_rolling_std_14', 'Sales_rolling_max_14', 'Sales_rolling_min_14', 'Sales_rolling_mean_30', 'Sales_rolling_std_30', 'Sales_rolling_max_30', 'Sales_rolling_min_30', 'Sales_ema_7', 'Sales_ema_14', 'Sales_ema_30']\n",
      "\n",
      "Date range: 2015-02-02 00:00:00 to 2018-12-30 00:00:00\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Sales</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>Quarter</th>\n",
       "      <th>WeekOfYear</th>\n",
       "      <th>IsWeekend</th>\n",
       "      <th>DayOfYear</th>\n",
       "      <th>Month_sin</th>\n",
       "      <th>Month_cos</th>\n",
       "      <th>DayOfWeek_sin</th>\n",
       "      <th>DayOfWeek_cos</th>\n",
       "      <th>Sales_lag_1</th>\n",
       "      <th>Sales_lag_3</th>\n",
       "      <th>Sales_lag_7</th>\n",
       "      <th>Sales_lag_14</th>\n",
       "      <th>Sales_lag_21</th>\n",
       "      <th>Sales_lag_30</th>\n",
       "      <th>Sales_rolling_mean_7</th>\n",
       "      <th>Sales_rolling_std_7</th>\n",
       "      <th>Sales_rolling_max_7</th>\n",
       "      <th>Sales_rolling_min_7</th>\n",
       "      <th>Sales_rolling_mean_14</th>\n",
       "      <th>Sales_rolling_std_14</th>\n",
       "      <th>Sales_rolling_max_14</th>\n",
       "      <th>Sales_rolling_min_14</th>\n",
       "      <th>Sales_rolling_mean_30</th>\n",
       "      <th>Sales_rolling_std_30</th>\n",
       "      <th>Sales_rolling_max_30</th>\n",
       "      <th>Sales_rolling_min_30</th>\n",
       "      <th>Sales_ema_7</th>\n",
       "      <th>Sales_ema_14</th>\n",
       "      <th>Sales_ema_30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-02-02</td>\n",
       "      <td>211.6460</td>\n",
       "      <td>3</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>0.8660</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>468.9000</td>\n",
       "      <td>240.5000</td>\n",
       "      <td>1097.2500</td>\n",
       "      <td>378.5940</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>16.4480</td>\n",
       "      <td>234.6157</td>\n",
       "      <td>184.2730</td>\n",
       "      <td>468.9000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>389.5364</td>\n",
       "      <td>724.2957</td>\n",
       "      <td>2673.8700</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>495.6602</td>\n",
       "      <td>1082.5475</td>\n",
       "      <td>4407.1000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>285.4799</td>\n",
       "      <td>325.3723</td>\n",
       "      <td>391.6922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-02-03</td>\n",
       "      <td>97.1120</td>\n",
       "      <td>2</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>0.8660</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.7818</td>\n",
       "      <td>0.6235</td>\n",
       "      <td>211.6460</td>\n",
       "      <td>290.6660</td>\n",
       "      <td>426.6700</td>\n",
       "      <td>2673.8700</td>\n",
       "      <td>3553.7950</td>\n",
       "      <td>288.0600</td>\n",
       "      <td>187.5360</td>\n",
       "      <td>168.4469</td>\n",
       "      <td>468.9000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>205.4823</td>\n",
       "      <td>305.4535</td>\n",
       "      <td>1097.2500</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>489.2952</td>\n",
       "      <td>1084.3700</td>\n",
       "      <td>4407.1000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>238.3832</td>\n",
       "      <td>294.6220</td>\n",
       "      <td>370.1358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-02-04</td>\n",
       "      <td>134.3840</td>\n",
       "      <td>3</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>0.8660</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.9749</td>\n",
       "      <td>-0.2225</td>\n",
       "      <td>97.1120</td>\n",
       "      <td>468.9000</td>\n",
       "      <td>3.9280</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>61.9600</td>\n",
       "      <td>19.5360</td>\n",
       "      <td>206.1726</td>\n",
       "      <td>151.0674</td>\n",
       "      <td>468.9000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>215.0811</td>\n",
       "      <td>300.5720</td>\n",
       "      <td>1097.2500</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>493.1235</td>\n",
       "      <td>1082.8561</td>\n",
       "      <td>4407.1000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>212.3814</td>\n",
       "      <td>273.0652</td>\n",
       "      <td>353.0325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-02-05</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>0.8660</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.4339</td>\n",
       "      <td>-0.9010</td>\n",
       "      <td>134.3840</td>\n",
       "      <td>211.6460</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>149.9500</td>\n",
       "      <td>4407.1000</td>\n",
       "      <td>206.1726</td>\n",
       "      <td>151.0674</td>\n",
       "      <td>468.9000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>215.0811</td>\n",
       "      <td>300.5720</td>\n",
       "      <td>1097.2500</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>346.2202</td>\n",
       "      <td>793.9704</td>\n",
       "      <td>3553.7950</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>159.2831</td>\n",
       "      <td>236.3737</td>\n",
       "      <td>327.6247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-02-06</td>\n",
       "      <td>330.5120</td>\n",
       "      <td>4</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>0.8660</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>-0.4339</td>\n",
       "      <td>-0.9010</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>97.1120</td>\n",
       "      <td>240.5000</td>\n",
       "      <td>40.0800</td>\n",
       "      <td>299.9640</td>\n",
       "      <td>87.1580</td>\n",
       "      <td>219.0314</td>\n",
       "      <td>158.1416</td>\n",
       "      <td>468.9000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>235.8263</td>\n",
       "      <td>297.5722</td>\n",
       "      <td>1097.2500</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>354.3320</td>\n",
       "      <td>792.4741</td>\n",
       "      <td>3553.7950</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>202.0921</td>\n",
       "      <td>249.0099</td>\n",
       "      <td>327.8310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2015-02-07</td>\n",
       "      <td>180.3200</td>\n",
       "      <td>2</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>0.8660</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>-0.9749</td>\n",
       "      <td>-0.2225</td>\n",
       "      <td>330.5120</td>\n",
       "      <td>134.3840</td>\n",
       "      <td>290.6660</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>203.2677</td>\n",
       "      <td>155.2848</td>\n",
       "      <td>468.9000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>248.7063</td>\n",
       "      <td>290.3955</td>\n",
       "      <td>1097.2500</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>360.3426</td>\n",
       "      <td>790.3750</td>\n",
       "      <td>3553.7950</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>196.6489</td>\n",
       "      <td>239.7979</td>\n",
       "      <td>317.3656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2015-02-08</td>\n",
       "      <td>14.5600</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>0.8660</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>-0.7818</td>\n",
       "      <td>0.6235</td>\n",
       "      <td>180.3200</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>468.9000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>64.8640</td>\n",
       "      <td>40.5440</td>\n",
       "      <td>138.3620</td>\n",
       "      <td>115.6439</td>\n",
       "      <td>330.5120</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>249.7463</td>\n",
       "      <td>289.4609</td>\n",
       "      <td>1097.2500</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>359.4765</td>\n",
       "      <td>790.7517</td>\n",
       "      <td>3553.7950</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>151.1256</td>\n",
       "      <td>209.6147</td>\n",
       "      <td>296.0198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2015-02-09</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0.8660</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>14.5600</td>\n",
       "      <td>330.5120</td>\n",
       "      <td>211.6460</td>\n",
       "      <td>1097.2500</td>\n",
       "      <td>378.5940</td>\n",
       "      <td>54.8300</td>\n",
       "      <td>108.1269</td>\n",
       "      <td>120.8411</td>\n",
       "      <td>330.5120</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>171.3713</td>\n",
       "      <td>163.4608</td>\n",
       "      <td>468.9000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>357.6488</td>\n",
       "      <td>791.5430</td>\n",
       "      <td>3553.7950</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>113.3435</td>\n",
       "      <td>181.5440</td>\n",
       "      <td>275.2764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2015-02-10</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>0.8660</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.7818</td>\n",
       "      <td>0.6235</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>180.3200</td>\n",
       "      <td>97.1120</td>\n",
       "      <td>426.6700</td>\n",
       "      <td>2673.8700</td>\n",
       "      <td>9.9400</td>\n",
       "      <td>94.2537</td>\n",
       "      <td>127.6964</td>\n",
       "      <td>330.5120</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>140.8949</td>\n",
       "      <td>151.5408</td>\n",
       "      <td>468.9000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>357.3175</td>\n",
       "      <td>791.6957</td>\n",
       "      <td>3553.7950</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>85.0073</td>\n",
       "      <td>157.2466</td>\n",
       "      <td>256.0932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2015-02-11</td>\n",
       "      <td>2043.4000</td>\n",
       "      <td>9</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>0.8660</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.9749</td>\n",
       "      <td>-0.2225</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>14.5600</td>\n",
       "      <td>134.3840</td>\n",
       "      <td>3.9280</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>366.9703</td>\n",
       "      <td>749.9754</td>\n",
       "      <td>2043.4000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>286.5714</td>\n",
       "      <td>526.3958</td>\n",
       "      <td>2043.4000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>425.4308</td>\n",
       "      <td>845.9374</td>\n",
       "      <td>3553.7950</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>574.6104</td>\n",
       "      <td>409.5579</td>\n",
       "      <td>380.0046</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date     Sales  Quantity  Year  Month  Day  DayOfWeek  Quarter  \\\n",
       "0 2015-02-02  211.6460         3  2015      2    2          0        1   \n",
       "1 2015-02-03   97.1120         2  2015      2    3          1        1   \n",
       "2 2015-02-04  134.3840         3  2015      2    4          2        1   \n",
       "3 2015-02-05    0.0000         0  2015      2    5          3        1   \n",
       "4 2015-02-06  330.5120         4  2015      2    6          4        1   \n",
       "5 2015-02-07  180.3200         2  2015      2    7          5        1   \n",
       "6 2015-02-08   14.5600         1  2015      2    8          6        1   \n",
       "7 2015-02-09    0.0000         0  2015      2    9          0        1   \n",
       "8 2015-02-10    0.0000         0  2015      2   10          1        1   \n",
       "9 2015-02-11 2043.4000         9  2015      2   11          2        1   \n",
       "\n",
       "   WeekOfYear  IsWeekend  DayOfYear  Month_sin  Month_cos  DayOfWeek_sin  \\\n",
       "0           6          0         33     0.8660     0.5000         0.0000   \n",
       "1           6          0         34     0.8660     0.5000         0.7818   \n",
       "2           6          0         35     0.8660     0.5000         0.9749   \n",
       "3           6          0         36     0.8660     0.5000         0.4339   \n",
       "4           6          0         37     0.8660     0.5000        -0.4339   \n",
       "5           6          1         38     0.8660     0.5000        -0.9749   \n",
       "6           6          1         39     0.8660     0.5000        -0.7818   \n",
       "7           7          0         40     0.8660     0.5000         0.0000   \n",
       "8           7          0         41     0.8660     0.5000         0.7818   \n",
       "9           7          0         42     0.8660     0.5000         0.9749   \n",
       "\n",
       "   DayOfWeek_cos  Sales_lag_1  Sales_lag_3  Sales_lag_7  Sales_lag_14  \\\n",
       "0         1.0000     468.9000     240.5000    1097.2500      378.5940   \n",
       "1         0.6235     211.6460     290.6660     426.6700     2673.8700   \n",
       "2        -0.2225      97.1120     468.9000       3.9280        0.0000   \n",
       "3        -0.9010     134.3840     211.6460       0.0000        0.0000   \n",
       "4        -0.9010       0.0000      97.1120     240.5000       40.0800   \n",
       "5        -0.2225     330.5120     134.3840     290.6660        0.0000   \n",
       "6         0.6235     180.3200       0.0000     468.9000        0.0000   \n",
       "7         1.0000      14.5600     330.5120     211.6460     1097.2500   \n",
       "8         0.6235       0.0000     180.3200      97.1120      426.6700   \n",
       "9        -0.2225       0.0000      14.5600     134.3840        3.9280   \n",
       "\n",
       "   Sales_lag_21  Sales_lag_30  Sales_rolling_mean_7  Sales_rolling_std_7  \\\n",
       "0        0.0000       16.4480              234.6157             184.2730   \n",
       "1     3553.7950      288.0600              187.5360             168.4469   \n",
       "2       61.9600       19.5360              206.1726             151.0674   \n",
       "3      149.9500     4407.1000              206.1726             151.0674   \n",
       "4      299.9640       87.1580              219.0314             158.1416   \n",
       "5        0.0000        0.0000              203.2677             155.2848   \n",
       "6       64.8640       40.5440              138.3620             115.6439   \n",
       "7      378.5940       54.8300              108.1269             120.8411   \n",
       "8     2673.8700        9.9400               94.2537             127.6964   \n",
       "9        0.0000        0.0000              366.9703             749.9754   \n",
       "\n",
       "   Sales_rolling_max_7  Sales_rolling_min_7  Sales_rolling_mean_14  \\\n",
       "0             468.9000               0.0000               389.5364   \n",
       "1             468.9000               0.0000               205.4823   \n",
       "2             468.9000               0.0000               215.0811   \n",
       "3             468.9000               0.0000               215.0811   \n",
       "4             468.9000               0.0000               235.8263   \n",
       "5             468.9000               0.0000               248.7063   \n",
       "6             330.5120               0.0000               249.7463   \n",
       "7             330.5120               0.0000               171.3713   \n",
       "8             330.5120               0.0000               140.8949   \n",
       "9            2043.4000               0.0000               286.5714   \n",
       "\n",
       "   Sales_rolling_std_14  Sales_rolling_max_14  Sales_rolling_min_14  \\\n",
       "0              724.2957             2673.8700                0.0000   \n",
       "1              305.4535             1097.2500                0.0000   \n",
       "2              300.5720             1097.2500                0.0000   \n",
       "3              300.5720             1097.2500                0.0000   \n",
       "4              297.5722             1097.2500                0.0000   \n",
       "5              290.3955             1097.2500                0.0000   \n",
       "6              289.4609             1097.2500                0.0000   \n",
       "7              163.4608              468.9000                0.0000   \n",
       "8              151.5408              468.9000                0.0000   \n",
       "9              526.3958             2043.4000                0.0000   \n",
       "\n",
       "   Sales_rolling_mean_30  Sales_rolling_std_30  Sales_rolling_max_30  \\\n",
       "0               495.6602             1082.5475             4407.1000   \n",
       "1               489.2952             1084.3700             4407.1000   \n",
       "2               493.1235             1082.8561             4407.1000   \n",
       "3               346.2202              793.9704             3553.7950   \n",
       "4               354.3320              792.4741             3553.7950   \n",
       "5               360.3426              790.3750             3553.7950   \n",
       "6               359.4765              790.7517             3553.7950   \n",
       "7               357.6488              791.5430             3553.7950   \n",
       "8               357.3175              791.6957             3553.7950   \n",
       "9               425.4308              845.9374             3553.7950   \n",
       "\n",
       "   Sales_rolling_min_30  Sales_ema_7  Sales_ema_14  Sales_ema_30  \n",
       "0                0.0000     285.4799      325.3723      391.6922  \n",
       "1                0.0000     238.3832      294.6220      370.1358  \n",
       "2                0.0000     212.3814      273.0652      353.0325  \n",
       "3                0.0000     159.2831      236.3737      327.6247  \n",
       "4                0.0000     202.0921      249.0099      327.8310  \n",
       "5                0.0000     196.6489      239.7979      317.3656  \n",
       "6                0.0000     151.1256      209.6147      296.0198  \n",
       "7                0.0000     113.3435      181.5440      275.2764  \n",
       "8                0.0000      85.0073      157.2466      256.0932  \n",
       "9                0.0000     574.6104      409.5579      380.0046  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 6. Sales Data - Preprocessing & Feature Engineering\n",
    "# ============================================================\n",
    "\n",
    "# Aggregate daily sales\n",
    "daily_df = sales_df.groupby('Order Date').agg(\n",
    "    Sales=('Sales', 'sum'),\n",
    "    Quantity=('Sales', 'count')\n",
    ").reset_index()\n",
    "daily_df = daily_df.sort_values('Order Date').reset_index(drop=True)\n",
    "\n",
    "# Fill missing dates with 0 sales\n",
    "date_range = pd.date_range(start=daily_df['Order Date'].min(), end=daily_df['Order Date'].max())\n",
    "daily_df = daily_df.set_index('Order Date').reindex(date_range, fill_value=0).reset_index()\n",
    "daily_df.columns = ['Date', 'Sales', 'Quantity']\n",
    "\n",
    "# Time features\n",
    "daily_df['Year'] = daily_df['Date'].dt.year\n",
    "daily_df['Month'] = daily_df['Date'].dt.month\n",
    "daily_df['Day'] = daily_df['Date'].dt.day\n",
    "daily_df['DayOfWeek'] = daily_df['Date'].dt.dayofweek\n",
    "daily_df['Quarter'] = daily_df['Date'].dt.quarter\n",
    "daily_df['WeekOfYear'] = daily_df['Date'].dt.isocalendar().week.astype(int)\n",
    "daily_df['IsWeekend'] = (daily_df['DayOfWeek'] >= 5).astype(int)\n",
    "daily_df['DayOfYear'] = daily_df['Date'].dt.dayofyear\n",
    "\n",
    "# Cyclical encoding for month and day of week\n",
    "daily_df['Month_sin'] = np.sin(2 * np.pi * daily_df['Month'] / 12)\n",
    "daily_df['Month_cos'] = np.cos(2 * np.pi * daily_df['Month'] / 12)\n",
    "daily_df['DayOfWeek_sin'] = np.sin(2 * np.pi * daily_df['DayOfWeek'] / 7)\n",
    "daily_df['DayOfWeek_cos'] = np.cos(2 * np.pi * daily_df['DayOfWeek'] / 7)\n",
    "\n",
    "# Lag features\n",
    "for lag in [1, 3, 7, 14, 21, 30]:\n",
    "    daily_df[f'Sales_lag_{lag}'] = daily_df['Sales'].shift(lag)\n",
    "\n",
    "# Rolling window statistics\n",
    "for window in [7, 14, 30]:\n",
    "    daily_df[f'Sales_rolling_mean_{window}'] = daily_df['Sales'].rolling(window=window, min_periods=1).mean()\n",
    "    daily_df[f'Sales_rolling_std_{window}'] = daily_df['Sales'].rolling(window=window, min_periods=1).std()\n",
    "    daily_df[f'Sales_rolling_max_{window}'] = daily_df['Sales'].rolling(window=window, min_periods=1).max()\n",
    "    daily_df[f'Sales_rolling_min_{window}'] = daily_df['Sales'].rolling(window=window, min_periods=1).min()\n",
    "\n",
    "# Exponential moving averages\n",
    "for span in [7, 14, 30]:\n",
    "    daily_df[f'Sales_ema_{span}'] = daily_df['Sales'].ewm(span=span).mean()\n",
    "\n",
    "# Drop rows with NaN (from lag features)\n",
    "daily_df = daily_df.dropna().reset_index(drop=True)\n",
    "\n",
    "print(f\"‚úÖ Feature engineering complete!\")\n",
    "print(f\"Dataset shape: {daily_df.shape}\")\n",
    "print(f\"Features: {list(daily_df.columns)}\")\n",
    "print(f\"\\nDate range: {daily_df['Date'].min()} to {daily_df['Date'].max()}\")\n",
    "daily_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76b5e989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Train/Test Split Complete\n",
      "Training set: 1142 samples\n",
      "Test set: 286 samples\n",
      "Features: 34\n",
      "Train date range: 2015-02-02 00:00:00 to 2018-03-19 00:00:00\n",
      "Test date range: 2018-03-20 00:00:00 to 2018-12-30 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Train/Test Split (Temporal - no data leakage)\n",
    "# ============================================================\n",
    "feature_cols = [c for c in daily_df.columns if c not in ['Date', 'Sales']]\n",
    "X = daily_df[feature_cols].values\n",
    "y = daily_df['Sales'].values\n",
    "\n",
    "# Use last 20% as test set (temporal split)\n",
    "split_idx = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "dates_test = daily_df['Date'].values[split_idx:]\n",
    "\n",
    "# Scale features\n",
    "scaler_X = StandardScaler()\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "\n",
    "print(f\"‚úÖ Train/Test Split Complete\")\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"Features: {X_train.shape[1]}\")\n",
    "print(f\"Train date range: {daily_df['Date'].iloc[0]} to {daily_df['Date'].iloc[split_idx-1]}\")\n",
    "print(f\"Test date range: {daily_df['Date'].iloc[split_idx]} to {daily_df['Date'].iloc[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6998fe",
   "metadata": {},
   "source": [
    "## 7. Sales Forecasting - Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee21af7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üå≤ Training Random Forest Regressor...\n",
      "\n",
      "‚úÖ Random Forest - Best Parameters: {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_depth': 20}\n",
      "   MAE:  $971.37\n",
      "   RMSE: $1547.21\n",
      "   R¬≤:   0.6228\n",
      "   MAPE: 21246926644.18%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 7. Random Forest Regressor for Sales Forecasting\n",
    "# ============================================================\n",
    "print(\"üå≤ Training Random Forest Regressor...\")\n",
    "\n",
    "# Hyperparameter tuning\n",
    "rf_params = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 15, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "rf_model = RandomForestRegressor(random_state=RANDOM_STATE, n_jobs=-1)\n",
    "rf_search = RandomizedSearchCV(rf_model, rf_params, n_iter=20, cv=3, scoring='neg_mean_absolute_error',\n",
    "                                random_state=RANDOM_STATE, n_jobs=-1, verbose=0)\n",
    "rf_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "rf_best = rf_search.best_estimator_\n",
    "rf_pred = rf_best.predict(X_test_scaled)\n",
    "\n",
    "# Metrics\n",
    "rf_mae = mean_absolute_error(y_test, rf_pred)\n",
    "rf_rmse = np.sqrt(mean_squared_error(y_test, rf_pred))\n",
    "rf_r2 = r2_score(y_test, rf_pred)\n",
    "rf_mape = np.mean(np.abs((y_test - rf_pred) / (y_test + 1e-8))) * 100\n",
    "\n",
    "print(f\"\\n‚úÖ Random Forest - Best Parameters: {rf_search.best_params_}\")\n",
    "print(f\"   MAE:  ${rf_mae:.2f}\")\n",
    "print(f\"   RMSE: ${rf_rmse:.2f}\")\n",
    "print(f\"   R¬≤:   {rf_r2:.4f}\")\n",
    "print(f\"   MAPE: {rf_mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ecfb22ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  üíæ Saved: /Users/usama/Desktop/dummy/charts/sales_forecasting/06_rf_sales_results.png\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Random Forest - Visualization Charts\n",
    "# ============================================================\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 14))\n",
    "\n",
    "# Actual vs Predicted\n",
    "axes[0,0].plot(dates_test, y_test, label='Actual', color='steelblue', linewidth=1.5)\n",
    "axes[0,0].plot(dates_test, rf_pred, label='Predicted', color='red', linewidth=1.5, alpha=0.8)\n",
    "axes[0,0].set_title('Random Forest: Actual vs Predicted Sales', fontsize=14, fontweight='bold')\n",
    "axes[0,0].set_xlabel('Date')\n",
    "axes[0,0].set_ylabel('Sales ($)')\n",
    "axes[0,0].legend(fontsize=12)\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Residual Plot\n",
    "residuals_rf = y_test - rf_pred\n",
    "axes[0,1].scatter(rf_pred, residuals_rf, alpha=0.5, color='steelblue', s=20)\n",
    "axes[0,1].axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[0,1].set_title('Random Forest: Residual Plot', fontsize=14, fontweight='bold')\n",
    "axes[0,1].set_xlabel('Predicted Sales ($)')\n",
    "axes[0,1].set_ylabel('Residuals ($)')\n",
    "\n",
    "# Prediction Error Distribution\n",
    "axes[1,0].hist(residuals_rf, bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "axes[1,0].axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[1,0].set_title('Random Forest: Prediction Error Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1,0].set_xlabel('Prediction Error ($)')\n",
    "axes[1,0].set_ylabel('Frequency')\n",
    "\n",
    "# Feature Importance\n",
    "importances = rf_best.feature_importances_\n",
    "feat_imp = pd.Series(importances, index=feature_cols).sort_values(ascending=True)\n",
    "feat_imp.tail(15).plot(kind='barh', ax=axes[1,1], color='steelblue', edgecolor='black')\n",
    "axes[1,1].set_title('Random Forest: Top 15 Feature Importances', fontsize=14, fontweight='bold')\n",
    "axes[1,1].set_xlabel('Importance')\n",
    "\n",
    "plt.tight_layout()\n",
    "save_chart(fig, '06_rf_sales_results.png', 'sales_forecasting')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5a27f8",
   "metadata": {},
   "source": [
    "## 8. Sales Forecasting - Gradient Boosting (XGBoost) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28a06ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Training XGBoost Regressor...\n",
      "\n",
      "‚úÖ XGBoost Results:\n",
      "   MAE:  $850.69\n",
      "   RMSE: $1374.24\n",
      "   R¬≤:   0.7024\n",
      "   MAPE: 97466863417.52%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 8. XGBoost Gradient Boosting for Sales Forecasting\n",
    "# ============================================================\n",
    "print(\"üöÄ Training XGBoost Regressor...\")\n",
    "\n",
    "# Split train into train/val for early stopping\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X_train_scaled, y_train, test_size=0.2, random_state=RANDOM_STATE)\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=500,\n",
    "    max_depth=8,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=1.0,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    "    verbosity=0\n",
    ")\n",
    "\n",
    "xgb_model.fit(\n",
    "    X_tr, y_tr,\n",
    "    eval_set=[(X_tr, y_tr), (X_val, y_val)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "xgb_pred = xgb_model.predict(X_test_scaled)\n",
    "\n",
    "# Metrics\n",
    "xgb_mae = mean_absolute_error(y_test, xgb_pred)\n",
    "xgb_rmse = np.sqrt(mean_squared_error(y_test, xgb_pred))\n",
    "xgb_r2 = r2_score(y_test, xgb_pred)\n",
    "xgb_mape = np.mean(np.abs((y_test - xgb_pred) / (y_test + 1e-8))) * 100\n",
    "\n",
    "print(f\"\\n‚úÖ XGBoost Results:\")\n",
    "print(f\"   MAE:  ${xgb_mae:.2f}\")\n",
    "print(f\"   RMSE: ${xgb_rmse:.2f}\")\n",
    "print(f\"   R¬≤:   {xgb_r2:.4f}\")\n",
    "print(f\"   MAPE: {xgb_mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97bd5614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  üíæ Saved: /Users/usama/Desktop/dummy/charts/sales_forecasting/07_xgb_sales_results.png\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# XGBoost - Visualization Charts\n",
    "# ============================================================\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 14))\n",
    "\n",
    "# Actual vs Predicted\n",
    "axes[0,0].plot(dates_test, y_test, label='Actual', color='steelblue', linewidth=1.5)\n",
    "axes[0,0].plot(dates_test, xgb_pred, label='Predicted (XGBoost)', color='orange', linewidth=1.5, alpha=0.8)\n",
    "axes[0,0].set_title('XGBoost: Actual vs Predicted Sales', fontsize=14, fontweight='bold')\n",
    "axes[0,0].set_xlabel('Date')\n",
    "axes[0,0].set_ylabel('Sales ($)')\n",
    "axes[0,0].legend(fontsize=12)\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Learning Curves\n",
    "evals_result = xgb_model.evals_result()\n",
    "axes[0,1].plot(evals_result['validation_0']['rmse'], label='Train RMSE', color='steelblue')\n",
    "axes[0,1].plot(evals_result['validation_1']['rmse'], label='Validation RMSE', color='orange')\n",
    "axes[0,1].set_title('XGBoost: Learning Curves', fontsize=14, fontweight='bold')\n",
    "axes[0,1].set_xlabel('Boosting Rounds')\n",
    "axes[0,1].set_ylabel('RMSE')\n",
    "axes[0,1].legend(fontsize=12)\n",
    "\n",
    "# Residual Analysis\n",
    "residuals_xgb = y_test - xgb_pred\n",
    "axes[1,0].scatter(xgb_pred, residuals_xgb, alpha=0.5, color='orange', s=20)\n",
    "axes[1,0].axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[1,0].set_title('XGBoost: Residual Plot', fontsize=14, fontweight='bold')\n",
    "axes[1,0].set_xlabel('Predicted Sales ($)')\n",
    "axes[1,0].set_ylabel('Residuals ($)')\n",
    "\n",
    "# Feature Importance (Gain-based)\n",
    "xgb_imp = pd.Series(xgb_model.feature_importances_, index=feature_cols).sort_values(ascending=True)\n",
    "xgb_imp.tail(15).plot(kind='barh', ax=axes[1,1], color='orange', edgecolor='black')\n",
    "axes[1,1].set_title('XGBoost: Top 15 Feature Importances', fontsize=14, fontweight='bold')\n",
    "axes[1,1].set_xlabel('Importance')\n",
    "\n",
    "plt.tight_layout()\n",
    "save_chart(fig, '07_xgb_sales_results.png', 'sales_forecasting')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd61d26f",
   "metadata": {},
   "source": [
    "## 9. Sales Forecasting - LSTM Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "50632be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Training LSTM Neural Network...\n",
      "\n",
      "‚úÖ LSTM Results:\n",
      "   MAE:  $1681.48\n",
      "   RMSE: $2377.96\n",
      "   R¬≤:   0.0378\n",
      "   MAPE: 1380347717865.57%\n",
      "\n",
      "Model Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ<span style=\"font-weight: bold\"> Layer (type)                    </span>‚îÉ<span style=\"font-weight: bold\"> Output Shape           </span>‚îÉ<span style=\"font-weight: bold\">       Param # </span>‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         ‚îÇ        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,896</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             ‚îÇ        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,416</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             ‚îÇ           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              ‚îÇ            <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)         ‚îÇ        \u001b[38;5;34m16,896\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout (\u001b[38;5;33mDropout\u001b[0m)               ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)         ‚îÇ             \u001b[38;5;34m0\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             ‚îÇ        \u001b[38;5;34m12,416\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             ‚îÇ             \u001b[38;5;34m0\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense (\u001b[38;5;33mDense\u001b[0m)                   ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             ‚îÇ           \u001b[38;5;34m528\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              ‚îÇ            \u001b[38;5;34m17\u001b[0m ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">89,573</span> (349.90 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m89,573\u001b[0m (349.90 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">29,857</span> (116.63 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m29,857\u001b[0m (116.63 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">59,716</span> (233.27 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m59,716\u001b[0m (233.27 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 9. LSTM Deep Learning for Sales Forecasting\n",
    "# ============================================================\n",
    "print(\"üß† Training LSTM Neural Network...\")\n",
    "\n",
    "# Prepare sequence data\n",
    "scaler_lstm = MinMaxScaler()\n",
    "sales_values = daily_df['Sales'].values.reshape(-1, 1)\n",
    "sales_scaled = scaler_lstm.fit_transform(sales_values)\n",
    "\n",
    "LOOKBACK = 30\n",
    "\n",
    "def create_sequences(data, lookback):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(lookback, len(data)):\n",
    "        X_seq.append(data[i-lookback:i, 0])\n",
    "        y_seq.append(data[i, 0])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "X_lstm, y_lstm = create_sequences(sales_scaled, LOOKBACK)\n",
    "X_lstm = X_lstm.reshape((X_lstm.shape[0], X_lstm.shape[1], 1))\n",
    "\n",
    "# Temporal split\n",
    "split_lstm = int(len(X_lstm) * 0.8)\n",
    "X_lstm_train, X_lstm_test = X_lstm[:split_lstm], X_lstm[split_lstm:]\n",
    "y_lstm_train, y_lstm_test = y_lstm[:split_lstm], y_lstm[split_lstm:]\n",
    "\n",
    "# Build LSTM Model\n",
    "lstm_model = Sequential([\n",
    "    Input(shape=(LOOKBACK, 1)),\n",
    "    LSTM(64, return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(32, return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "lstm_model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=0)\n",
    "]\n",
    "\n",
    "history = lstm_model.fit(\n",
    "    X_lstm_train, y_lstm_train,\n",
    "    validation_data=(X_lstm_test, y_lstm_test),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Predict & inverse transform\n",
    "lstm_pred_scaled = lstm_model.predict(X_lstm_test, verbose=0)\n",
    "lstm_pred = scaler_lstm.inverse_transform(lstm_pred_scaled).flatten()\n",
    "y_lstm_actual = scaler_lstm.inverse_transform(y_lstm_test.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Metrics\n",
    "lstm_mae = mean_absolute_error(y_lstm_actual, lstm_pred)\n",
    "lstm_rmse = np.sqrt(mean_squared_error(y_lstm_actual, lstm_pred))\n",
    "lstm_r2 = r2_score(y_lstm_actual, lstm_pred)\n",
    "lstm_mape = np.mean(np.abs((y_lstm_actual - lstm_pred) / (y_lstm_actual + 1e-8))) * 100\n",
    "\n",
    "print(f\"\\n‚úÖ LSTM Results:\")\n",
    "print(f\"   MAE:  ${lstm_mae:.2f}\")\n",
    "print(f\"   RMSE: ${lstm_rmse:.2f}\")\n",
    "print(f\"   R¬≤:   {lstm_r2:.4f}\")\n",
    "print(f\"   MAPE: {lstm_mape:.2f}%\")\n",
    "print(f\"\\nModel Summary:\")\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef795d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  üíæ Saved: /Users/usama/Desktop/dummy/charts/sales_forecasting/08_lstm_sales_results.png\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# LSTM - Visualization Charts\n",
    "# ============================================================\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 14))\n",
    "\n",
    "# Training/Validation Loss\n",
    "axes[0,0].plot(history.history['loss'], label='Training Loss', color='steelblue', linewidth=2)\n",
    "axes[0,0].plot(history.history['val_loss'], label='Validation Loss', color='red', linewidth=2)\n",
    "axes[0,0].set_title('LSTM: Training & Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[0,0].set_xlabel('Epoch')\n",
    "axes[0,0].set_ylabel('Loss (MSE)')\n",
    "axes[0,0].legend(fontsize=12)\n",
    "\n",
    "# Actual vs Predicted\n",
    "axes[0,1].plot(y_lstm_actual, label='Actual', color='steelblue', linewidth=1.5)\n",
    "axes[0,1].plot(lstm_pred, label='Predicted (LSTM)', color='green', linewidth=1.5, alpha=0.8)\n",
    "axes[0,1].set_title('LSTM: Actual vs Predicted Sales', fontsize=14, fontweight='bold')\n",
    "axes[0,1].set_xlabel('Time Steps')\n",
    "axes[0,1].set_ylabel('Sales ($)')\n",
    "axes[0,1].legend(fontsize=12)\n",
    "\n",
    "# Residual Plot\n",
    "residuals_lstm = y_lstm_actual - lstm_pred\n",
    "axes[1,0].scatter(range(len(residuals_lstm)), residuals_lstm, alpha=0.5, color='green', s=20)\n",
    "axes[1,0].axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[1,0].set_title('LSTM: Residual Plot', fontsize=14, fontweight='bold')\n",
    "axes[1,0].set_xlabel('Sample Index')\n",
    "axes[1,0].set_ylabel('Residuals ($)')\n",
    "\n",
    "# Training MAE\n",
    "axes[1,1].plot(history.history['mae'], label='Training MAE', color='steelblue', linewidth=2)\n",
    "axes[1,1].plot(history.history['val_mae'], label='Validation MAE', color='red', linewidth=2)\n",
    "axes[1,1].set_title('LSTM: Training & Validation MAE', fontsize=14, fontweight='bold')\n",
    "axes[1,1].set_xlabel('Epoch')\n",
    "axes[1,1].set_ylabel('MAE')\n",
    "axes[1,1].legend(fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "save_chart(fig, '08_lstm_sales_results.png', 'sales_forecasting')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167d2604",
   "metadata": {},
   "source": [
    "## 10. Sales Forecasting - Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d36c8b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üìä SALES FORECASTING - MODEL COMPARISON\n",
      "======================================================================\n",
      "        Model       MAE      RMSE     R¬≤           MAPE (%)\n",
      "Random Forest  971.3678 1547.2128 0.6228   21246926644.1755\n",
      "      XGBoost  850.6866 1374.2355 0.7024   97466863417.5179\n",
      "         LSTM 1681.4763 2377.9608 0.0378 1380347717865.5696\n",
      "\n",
      "üèÜ Best Model: XGBoost (lowest MAE)\n",
      "  üíæ Saved: /Users/usama/Desktop/dummy/charts/comparison/09_sales_radar_comparison.png\n",
      "  üíæ Saved: /Users/usama/Desktop/dummy/charts/comparison/10_sales_error_boxplot.png\n",
      "  üíæ Saved: /Users/usama/Desktop/dummy/charts/comparison/09_sales_model_comparison.png\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 10. Sales Forecasting Model Comparison\n",
    "# ============================================================\n",
    "sales_comparison = pd.DataFrame({\n",
    "    'Model': ['Random Forest', 'XGBoost', 'LSTM'],\n",
    "    'MAE': [rf_mae, xgb_mae, lstm_mae],\n",
    "    'RMSE': [rf_rmse, xgb_rmse, lstm_rmse],\n",
    "    'R¬≤': [rf_r2, xgb_r2, lstm_r2],\n",
    "    'MAPE (%)': [rf_mape, xgb_mape, lstm_mape]\n",
    "})\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üìä SALES FORECASTING - MODEL COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "print(sales_comparison.to_string(index=False))\n",
    "print(f\"\\nüèÜ Best Model: {sales_comparison.loc[sales_comparison['MAE'].idxmin(), 'Model']} (lowest MAE)\")\n",
    "\n",
    "# Comparison Charts\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 14))\n",
    "\n",
    "# Grouped Bar Chart - Metrics Comparison\n",
    "metrics = ['MAE', 'RMSE']\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.25\n",
    "for i, model in enumerate(sales_comparison['Model']):\n",
    "    values = [sales_comparison.loc[i, 'MAE'], sales_comparison.loc[i, 'RMSE']]\n",
    "    axes[0,0].bar(x + i*width, values, width, label=model, edgecolor='black', alpha=0.8)\n",
    "axes[0,0].set_xticks(x + width)\n",
    "axes[0,0].set_xticklabels(metrics)\n",
    "axes[0,0].set_title('Model Comparison: MAE & RMSE', fontsize=14, fontweight='bold')\n",
    "axes[0,0].set_ylabel('Error ($)')\n",
    "axes[0,0].legend()\n",
    "\n",
    "# R¬≤ Score Comparison\n",
    "colors = ['steelblue', 'orange', 'green']\n",
    "axes[0,1].bar(sales_comparison['Model'], sales_comparison['R¬≤'], color=colors, edgecolor='black', alpha=0.8)\n",
    "axes[0,1].set_title('Model Comparison: R¬≤ Score', fontsize=14, fontweight='bold')\n",
    "axes[0,1].set_ylabel('R¬≤ Score')\n",
    "for i, v in enumerate(sales_comparison['R¬≤']):\n",
    "    axes[0,1].text(i, v + 0.01, f'{v:.4f}', ha='center', fontweight='bold')\n",
    "\n",
    "# All Models Predictions Overlay (using RF & XGB test data)\n",
    "axes[1,0].plot(dates_test, y_test, label='Actual', color='black', linewidth=2)\n",
    "axes[1,0].plot(dates_test, rf_pred, label='Random Forest', color='steelblue', linewidth=1.5, alpha=0.7)\n",
    "axes[1,0].plot(dates_test, xgb_pred, label='XGBoost', color='orange', linewidth=1.5, alpha=0.7)\n",
    "axes[1,0].set_title('All Sales Models: Predictions Overlay', fontsize=14, fontweight='bold')\n",
    "axes[1,0].set_xlabel('Date')\n",
    "axes[1,0].set_ylabel('Sales ($)')\n",
    "axes[1,0].legend(fontsize=10)\n",
    "axes[1,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Radar Chart\n",
    "categories = ['MAE\\n(inverted)', 'RMSE\\n(inverted)', 'R¬≤', 'MAPE\\n(inverted)']\n",
    "fig_radar = plt.figure(figsize=(8, 8))\n",
    "ax_radar = fig_radar.add_subplot(111, polar=True)\n",
    "angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()\n",
    "angles += angles[:1]\n",
    "\n",
    "for i, model in enumerate(sales_comparison['Model']):\n",
    "    # Normalize: higher is better, so invert error metrics\n",
    "    max_mae = sales_comparison['MAE'].max()\n",
    "    max_rmse = sales_comparison['RMSE'].max()\n",
    "    max_mape = sales_comparison['MAPE (%)'].max()\n",
    "    values = [\n",
    "        1 - sales_comparison.loc[i, 'MAE'] / (max_mae + 1e-8),\n",
    "        1 - sales_comparison.loc[i, 'RMSE'] / (max_rmse + 1e-8),\n",
    "        max(sales_comparison.loc[i, 'R¬≤'], 0),\n",
    "        1 - sales_comparison.loc[i, 'MAPE (%)'] / (max_mape + 1e-8)\n",
    "    ]\n",
    "    values += values[:1]\n",
    "    ax_radar.plot(angles, values, 'o-', linewidth=2, label=model)\n",
    "    ax_radar.fill(angles, values, alpha=0.1)\n",
    "\n",
    "ax_radar.set_xticks(angles[:-1])\n",
    "ax_radar.set_xticklabels(categories, fontsize=10)\n",
    "ax_radar.set_title('Sales Models: Radar Comparison', fontsize=14, fontweight='bold', pad=20)\n",
    "ax_radar.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
    "save_chart(fig_radar, '09_sales_radar_comparison.png', 'comparison')\n",
    "plt.show()\n",
    "\n",
    "# Box plot of prediction errors\n",
    "fig_box, ax_box = plt.subplots(figsize=(10, 6))\n",
    "errors_data = pd.DataFrame({\n",
    "    'Random Forest': residuals_rf,\n",
    "    'XGBoost': residuals_xgb[:len(residuals_rf)]\n",
    "})\n",
    "errors_data.boxplot(ax=ax_box, patch_artist=True,\n",
    "                     boxprops=dict(facecolor='lightblue'), medianprops=dict(color='red', linewidth=2))\n",
    "ax_box.set_title('Prediction Error Distribution by Model', fontsize=14, fontweight='bold')\n",
    "ax_box.set_ylabel('Prediction Error ($)')\n",
    "ax_box.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "save_chart(fig_box, '10_sales_error_boxplot.png', 'comparison')\n",
    "plt.show()\n",
    "\n",
    "save_chart(fig, '09_sales_model_comparison.png', 'comparison')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c026e8e1",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 2: CUSTOMER CHURN PREDICTION\n",
    "---\n",
    "## 11. Data Preprocessing & Feature Engineering for Churn Prediction\n",
    "\n",
    "Encode categorical variables, handle class imbalance with SMOTE, engineer new features, and prepare train/test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "314d552e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Churn data preprocessed!\n",
      "Shape: (7043, 41)\n",
      "Target distribution:\n",
      "Churn\n",
      "0    5174\n",
      "1    1869\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>SeniorCitizen</th>\n",
       "      <th>Partner</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>tenure</th>\n",
       "      <th>PhoneService</th>\n",
       "      <th>PaperlessBilling</th>\n",
       "      <th>MonthlyCharges</th>\n",
       "      <th>TotalCharges</th>\n",
       "      <th>Churn</th>\n",
       "      <th>AvgMonthlySpend</th>\n",
       "      <th>ServiceCount</th>\n",
       "      <th>ContractValue</th>\n",
       "      <th>Tenure_x_Monthly</th>\n",
       "      <th>CLV_Estimate</th>\n",
       "      <th>MultipleLines_No phone service</th>\n",
       "      <th>MultipleLines_Yes</th>\n",
       "      <th>InternetService_Fiber optic</th>\n",
       "      <th>InternetService_No</th>\n",
       "      <th>OnlineSecurity_No internet service</th>\n",
       "      <th>OnlineSecurity_Yes</th>\n",
       "      <th>OnlineBackup_No internet service</th>\n",
       "      <th>OnlineBackup_Yes</th>\n",
       "      <th>DeviceProtection_No internet service</th>\n",
       "      <th>DeviceProtection_Yes</th>\n",
       "      <th>TechSupport_No internet service</th>\n",
       "      <th>TechSupport_Yes</th>\n",
       "      <th>StreamingTV_No internet service</th>\n",
       "      <th>StreamingTV_Yes</th>\n",
       "      <th>StreamingMovies_No internet service</th>\n",
       "      <th>StreamingMovies_Yes</th>\n",
       "      <th>Contract_One year</th>\n",
       "      <th>Contract_Two year</th>\n",
       "      <th>PaymentMethod_Credit card (automatic)</th>\n",
       "      <th>PaymentMethod_Electronic check</th>\n",
       "      <th>PaymentMethod_Mailed check</th>\n",
       "      <th>TenureGroup_12-24</th>\n",
       "      <th>TenureGroup_24-36</th>\n",
       "      <th>TenureGroup_36-48</th>\n",
       "      <th>TenureGroup_48-60</th>\n",
       "      <th>TenureGroup_60-72</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>29.8500</td>\n",
       "      <td>29.8500</td>\n",
       "      <td>0</td>\n",
       "      <td>14.9250</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>29.8500</td>\n",
       "      <td>358.2000</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>56.9500</td>\n",
       "      <td>1889.5000</td>\n",
       "      <td>0</td>\n",
       "      <td>53.9857</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1936.3000</td>\n",
       "      <td>23235.6000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>53.8500</td>\n",
       "      <td>108.1500</td>\n",
       "      <td>1</td>\n",
       "      <td>36.0500</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>107.7000</td>\n",
       "      <td>1292.4000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>42.3000</td>\n",
       "      <td>1840.7500</td>\n",
       "      <td>0</td>\n",
       "      <td>40.0163</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1903.5000</td>\n",
       "      <td>22842.0000</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>70.7000</td>\n",
       "      <td>151.6500</td>\n",
       "      <td>1</td>\n",
       "      <td>50.5500</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>141.4000</td>\n",
       "      <td>1696.8000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender  SeniorCitizen  Partner  Dependents  tenure  PhoneService  \\\n",
       "0       0              0        1           0       1             0   \n",
       "1       1              0        0           0      34             1   \n",
       "2       1              0        0           0       2             1   \n",
       "3       1              0        0           0      45             0   \n",
       "4       0              0        0           0       2             1   \n",
       "\n",
       "   PaperlessBilling  MonthlyCharges  TotalCharges  Churn  AvgMonthlySpend  \\\n",
       "0                 1         29.8500       29.8500      0          14.9250   \n",
       "1                 0         56.9500     1889.5000      0          53.9857   \n",
       "2                 1         53.8500      108.1500      1          36.0500   \n",
       "3                 0         42.3000     1840.7500      0          40.0163   \n",
       "4                 1         70.7000      151.6500      1          50.5500   \n",
       "\n",
       "   ServiceCount  ContractValue  Tenure_x_Monthly  CLV_Estimate  \\\n",
       "0             2              0           29.8500      358.2000   \n",
       "1             4              1         1936.3000    23235.6000   \n",
       "2             4              0          107.7000     1292.4000   \n",
       "3             4              1         1903.5000    22842.0000   \n",
       "4             2              0          141.4000     1696.8000   \n",
       "\n",
       "   MultipleLines_No phone service  MultipleLines_Yes  \\\n",
       "0                            True              False   \n",
       "1                           False              False   \n",
       "2                           False              False   \n",
       "3                            True              False   \n",
       "4                           False              False   \n",
       "\n",
       "   InternetService_Fiber optic  InternetService_No  \\\n",
       "0                        False               False   \n",
       "1                        False               False   \n",
       "2                        False               False   \n",
       "3                        False               False   \n",
       "4                         True               False   \n",
       "\n",
       "   OnlineSecurity_No internet service  OnlineSecurity_Yes  \\\n",
       "0                               False               False   \n",
       "1                               False                True   \n",
       "2                               False                True   \n",
       "3                               False                True   \n",
       "4                               False               False   \n",
       "\n",
       "   OnlineBackup_No internet service  OnlineBackup_Yes  \\\n",
       "0                             False              True   \n",
       "1                             False             False   \n",
       "2                             False              True   \n",
       "3                             False             False   \n",
       "4                             False             False   \n",
       "\n",
       "   DeviceProtection_No internet service  DeviceProtection_Yes  \\\n",
       "0                                 False                 False   \n",
       "1                                 False                  True   \n",
       "2                                 False                 False   \n",
       "3                                 False                  True   \n",
       "4                                 False                 False   \n",
       "\n",
       "   TechSupport_No internet service  TechSupport_Yes  \\\n",
       "0                            False            False   \n",
       "1                            False            False   \n",
       "2                            False            False   \n",
       "3                            False             True   \n",
       "4                            False            False   \n",
       "\n",
       "   StreamingTV_No internet service  StreamingTV_Yes  \\\n",
       "0                            False            False   \n",
       "1                            False            False   \n",
       "2                            False            False   \n",
       "3                            False            False   \n",
       "4                            False            False   \n",
       "\n",
       "   StreamingMovies_No internet service  StreamingMovies_Yes  \\\n",
       "0                                False                False   \n",
       "1                                False                False   \n",
       "2                                False                False   \n",
       "3                                False                False   \n",
       "4                                False                False   \n",
       "\n",
       "   Contract_One year  Contract_Two year  \\\n",
       "0              False              False   \n",
       "1               True              False   \n",
       "2              False              False   \n",
       "3               True              False   \n",
       "4              False              False   \n",
       "\n",
       "   PaymentMethod_Credit card (automatic)  PaymentMethod_Electronic check  \\\n",
       "0                                  False                            True   \n",
       "1                                  False                           False   \n",
       "2                                  False                           False   \n",
       "3                                  False                           False   \n",
       "4                                  False                            True   \n",
       "\n",
       "   PaymentMethod_Mailed check  TenureGroup_12-24  TenureGroup_24-36  \\\n",
       "0                       False              False              False   \n",
       "1                        True              False               True   \n",
       "2                        True              False              False   \n",
       "3                       False              False              False   \n",
       "4                       False              False              False   \n",
       "\n",
       "   TenureGroup_36-48  TenureGroup_48-60  TenureGroup_60-72  \n",
       "0              False              False              False  \n",
       "1              False              False              False  \n",
       "2              False              False              False  \n",
       "3               True              False              False  \n",
       "4              False              False              False  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 11. Churn Data - Preprocessing & Feature Engineering\n",
    "# ============================================================\n",
    "\n",
    "churn_processed = churn_df.copy()\n",
    "\n",
    "# Drop customerID (not useful for prediction)\n",
    "churn_processed = churn_processed.drop('customerID', axis=1)\n",
    "\n",
    "# Convert TotalCharges to numeric (already done, but ensure)\n",
    "churn_processed['TotalCharges'] = pd.to_numeric(churn_processed['TotalCharges'], errors='coerce')\n",
    "churn_processed['TotalCharges'].fillna(churn_processed['TotalCharges'].median(), inplace=True)\n",
    "\n",
    "# Feature Engineering\n",
    "# Tenure groups\n",
    "churn_processed['TenureGroup'] = pd.cut(churn_processed['tenure'], \n",
    "                                         bins=[0, 12, 24, 36, 48, 60, 72],\n",
    "                                         labels=['0-12', '12-24', '24-36', '36-48', '48-60', '60-72'])\n",
    "\n",
    "# Average monthly spend\n",
    "churn_processed['AvgMonthlySpend'] = churn_processed['TotalCharges'] / (churn_processed['tenure'] + 1)\n",
    "\n",
    "# Service count\n",
    "service_cols = ['PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', \n",
    "                'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies']\n",
    "churn_processed['ServiceCount'] = 0\n",
    "for col in service_cols:\n",
    "    if col in churn_processed.columns:\n",
    "        churn_processed['ServiceCount'] += (churn_processed[col].isin(['Yes', 'DSL', 'Fiber optic'])).astype(int)\n",
    "\n",
    "# Contract value indicator\n",
    "contract_map = {'Month-to-month': 0, 'One year': 1, 'Two year': 2}\n",
    "churn_processed['ContractValue'] = churn_processed['Contract'].map(contract_map)\n",
    "\n",
    "# Interaction feature\n",
    "churn_processed['Tenure_x_Monthly'] = churn_processed['tenure'] * churn_processed['MonthlyCharges']\n",
    "\n",
    "# Customer Lifetime Value estimate\n",
    "churn_processed['CLV_Estimate'] = churn_processed['tenure'] * churn_processed['MonthlyCharges'] * 12\n",
    "\n",
    "# Encode binary variables\n",
    "binary_cols = ['gender', 'Partner', 'Dependents', 'PhoneService', 'PaperlessBilling', 'Churn']\n",
    "le = LabelEncoder()\n",
    "for col in binary_cols:\n",
    "    churn_processed[col] = le.fit_transform(churn_processed[col])\n",
    "\n",
    "# One-hot encode multi-class categorical variables\n",
    "multi_cat_cols = ['MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup',\n",
    "                  'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies',\n",
    "                  'Contract', 'PaymentMethod', 'TenureGroup']\n",
    "churn_processed = pd.get_dummies(churn_processed, columns=multi_cat_cols, drop_first=True)\n",
    "\n",
    "# Ensure all columns are numeric\n",
    "for col in churn_processed.columns:\n",
    "    if churn_processed[col].dtype == 'object':\n",
    "        churn_processed[col] = le.fit_transform(churn_processed[col].astype(str))\n",
    "\n",
    "print(f\"‚úÖ Churn data preprocessed!\")\n",
    "print(f\"Shape: {churn_processed.shape}\")\n",
    "print(f\"Target distribution:\\n{churn_processed['Churn'].value_counts()}\")\n",
    "churn_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9df7f85e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nSMOTE does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Apply SMOTE to handle class imbalance\u001b[39;00m\n\u001b[32m     21\u001b[39m smote = SMOTE(random_state=RANDOM_STATE)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m X_train_smote, y_train_smote = \u001b[43msmote\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_c_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_c\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Train/Test Split Complete\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOriginal Training set: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_train_c.shape[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m samples\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/dummy/.venv/lib/python3.13/site-packages/imblearn/base.py:204\u001b[39m, in \u001b[36mBaseSampler.fit_resample\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    183\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit_resample\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, **params):\n\u001b[32m    184\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Resample the dataset.\u001b[39;00m\n\u001b[32m    185\u001b[39m \n\u001b[32m    186\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    202\u001b[39m \u001b[33;03m        The corresponding label of `X_resampled`.\u001b[39;00m\n\u001b[32m    203\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m204\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/dummy/.venv/lib/python3.13/site-packages/sklearn/base.py:1336\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1329\u001b[39m     estimator._validate_params()\n\u001b[32m   1331\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1332\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1333\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1334\u001b[39m     )\n\u001b[32m   1335\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1336\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/dummy/.venv/lib/python3.13/site-packages/imblearn/base.py:101\u001b[39m, in \u001b[36mSamplerMixin.fit_resample\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m     99\u001b[39m check_classification_targets(y)\n\u001b[32m    100\u001b[39m arrays_transformer = ArraysTransformer(X, y)\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m X, y, binarize_y = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_check_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[38;5;28mself\u001b[39m.sampling_strategy_ = check_sampling_strategy(\n\u001b[32m    104\u001b[39m     \u001b[38;5;28mself\u001b[39m.sampling_strategy, y, \u001b[38;5;28mself\u001b[39m._sampling_type\n\u001b[32m    105\u001b[39m )\n\u001b[32m    107\u001b[39m output = \u001b[38;5;28mself\u001b[39m._fit_resample(X, y, **params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/dummy/.venv/lib/python3.13/site-packages/imblearn/base.py:159\u001b[39m, in \u001b[36mBaseSampler._check_X_y\u001b[39m\u001b[34m(self, X, y, accept_sparse)\u001b[39m\n\u001b[32m    157\u001b[39m     accept_sparse = [\u001b[33m\"\u001b[39m\u001b[33mcsr\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcsc\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    158\u001b[39m y, binarize_y = check_target_type(y, indicate_one_vs_all=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m X, y = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m X, y, binarize_y\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/dummy/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2919\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2917\u001b[39m         y = check_array(y, input_name=\u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m, **check_y_params)\n\u001b[32m   2918\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2919\u001b[39m         X, y = \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2920\u001b[39m     out = X, y\n\u001b[32m   2922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params.get(\u001b[33m\"\u001b[39m\u001b[33mensure_2d\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/dummy/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:1314\u001b[39m, in \u001b[36mcheck_X_y\u001b[39m\u001b[34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[39m\n\u001b[32m   1309\u001b[39m         estimator_name = _check_estimator_name(estimator)\n\u001b[32m   1310\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1311\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m requires y to be passed, but the target y is None\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1312\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1314\u001b[39m X = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1315\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1316\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1317\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1318\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1319\u001b[39m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1320\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1321\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1322\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1324\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1326\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1327\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1328\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1329\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1331\u001b[39m y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)\n\u001b[32m   1333\u001b[39m check_consistent_length(X, y)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/dummy/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:1074\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1068\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1069\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound array with dim \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray.ndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1070\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m while dim <= 2 is required\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1071\u001b[39m     )\n\u001b[32m   1073\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[32m-> \u001b[39m\u001b[32m1074\u001b[39m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1075\u001b[39m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1076\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1077\u001b[39m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1078\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1079\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1081\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[32m   1082\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[32m   1083\u001b[39m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/dummy/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:133\u001b[39m, in \u001b[36m_assert_all_finite\u001b[39m\u001b[34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[32m    131\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/dummy/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:182\u001b[39m, in \u001b[36m_assert_all_finite_element_wise\u001b[39m\u001b[34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name == \u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[32m    168\u001b[39m     msg_err += (\n\u001b[32m    169\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not accept missing values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    170\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    180\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m#estimators-that-handle-nan-values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    181\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[31mValueError\u001b[39m: Input X contains NaN.\nSMOTE does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Train/Test Split with SMOTE for class imbalance\n",
    "# ============================================================\n",
    "X_churn = churn_processed.drop('Churn', axis=1)\n",
    "y_churn = churn_processed['Churn']\n",
    "\n",
    "# Store feature names\n",
    "churn_feature_names = X_churn.columns.tolist()\n",
    "\n",
    "# Stratified Train/Test Split\n",
    "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(\n",
    "    X_churn, y_churn, test_size=0.2, random_state=RANDOM_STATE, stratify=y_churn\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler_churn = StandardScaler()\n",
    "X_train_c_scaled = scaler_churn.fit_transform(X_train_c)\n",
    "X_test_c_scaled = scaler_churn.transform(X_test_c)\n",
    "\n",
    "# Apply SMOTE to handle class imbalance\n",
    "smote = SMOTE(random_state=RANDOM_STATE)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_c_scaled, y_train_c)\n",
    "\n",
    "print(f\"‚úÖ Train/Test Split Complete\")\n",
    "print(f\"Original Training set: {X_train_c.shape[0]} samples\")\n",
    "print(f\"After SMOTE: {X_train_smote.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test_c.shape[0]} samples\")\n",
    "print(f\"Features: {X_train_c.shape[1]}\")\n",
    "print(f\"\\nClass distribution after SMOTE:\")\n",
    "print(pd.Series(y_train_smote).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e57f96",
   "metadata": {},
   "source": [
    "## 12. Customer Churn - Logistic Regression Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c151516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 12. Logistic Regression Baseline for Churn\n",
    "# ============================================================\n",
    "print(\"üìä Training Logistic Regression...\")\n",
    "\n",
    "lr_model = LogisticRegression(max_iter=1000, class_weight='balanced', random_state=RANDOM_STATE)\n",
    "lr_model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "lr_pred = lr_model.predict(X_test_c_scaled)\n",
    "lr_prob = lr_model.predict_proba(X_test_c_scaled)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "lr_acc = accuracy_score(y_test_c, lr_pred)\n",
    "lr_prec = precision_score(y_test_c, lr_pred)\n",
    "lr_rec = recall_score(y_test_c, lr_pred)\n",
    "lr_f1 = f1_score(y_test_c, lr_pred)\n",
    "lr_auc = roc_auc_score(y_test_c, lr_prob)\n",
    "\n",
    "print(f\"\\n‚úÖ Logistic Regression Results:\")\n",
    "print(f\"   Accuracy:  {lr_acc:.4f}\")\n",
    "print(f\"   Precision: {lr_prec:.4f}\")\n",
    "print(f\"   Recall:    {lr_rec:.4f}\")\n",
    "print(f\"   F1-Score:  {lr_f1:.4f}\")\n",
    "print(f\"   AUC-ROC:   {lr_auc:.4f}\")\n",
    "print(f\"\\nClassification Report:\\n{classification_report(y_test_c, lr_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82867259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Logistic Regression - Visualization Charts\n",
    "# ============================================================\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_lr = confusion_matrix(y_test_c, lr_pred)\n",
    "sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues', ax=axes[0,0],\n",
    "            xticklabels=['No Churn', 'Churn'], yticklabels=['No Churn', 'Churn'])\n",
    "axes[0,0].set_title('Logistic Regression: Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "axes[0,0].set_xlabel('Predicted')\n",
    "axes[0,0].set_ylabel('Actual')\n",
    "\n",
    "# ROC Curve\n",
    "fpr_lr, tpr_lr, _ = roc_curve(y_test_c, lr_prob)\n",
    "axes[0,1].plot(fpr_lr, tpr_lr, color='steelblue', linewidth=2, label=f'AUC = {lr_auc:.4f}')\n",
    "axes[0,1].plot([0, 1], [0, 1], 'k--', linewidth=1)\n",
    "axes[0,1].set_title('Logistic Regression: ROC Curve', fontsize=14, fontweight='bold')\n",
    "axes[0,1].set_xlabel('False Positive Rate')\n",
    "axes[0,1].set_ylabel('True Positive Rate')\n",
    "axes[0,1].legend(fontsize=12)\n",
    "axes[0,1].fill_between(fpr_lr, tpr_lr, alpha=0.1, color='steelblue')\n",
    "\n",
    "# Precision-Recall Curve\n",
    "prec_lr, rec_lr, _ = precision_recall_curve(y_test_c, lr_prob)\n",
    "pr_auc_lr = auc(rec_lr, prec_lr)\n",
    "axes[1,0].plot(rec_lr, prec_lr, color='steelblue', linewidth=2, label=f'PR AUC = {pr_auc_lr:.4f}')\n",
    "axes[1,0].set_title('Logistic Regression: Precision-Recall Curve', fontsize=14, fontweight='bold')\n",
    "axes[1,0].set_xlabel('Recall')\n",
    "axes[1,0].set_ylabel('Precision')\n",
    "axes[1,0].legend(fontsize=12)\n",
    "\n",
    "# Feature Coefficients\n",
    "coefficients = pd.Series(np.abs(lr_model.coef_[0]), index=churn_feature_names).sort_values(ascending=True)\n",
    "coefficients.tail(15).plot(kind='barh', ax=axes[1,1], color='steelblue', edgecolor='black')\n",
    "axes[1,1].set_title('Logistic Regression: Top 15 Feature Importance (|coef|)', fontsize=14, fontweight='bold')\n",
    "axes[1,1].set_xlabel('|Coefficient|')\n",
    "\n",
    "plt.tight_layout()\n",
    "save_chart(fig, '05_lr_churn_results.png', 'customer_churn')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2bad5c",
   "metadata": {},
   "source": [
    "## 13. Customer Churn - Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49f5366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 13. Random Forest Classifier for Churn\n",
    "# ============================================================\n",
    "print(\"üå≤ Training Random Forest Classifier...\")\n",
    "\n",
    "rf_c_params = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 15, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "rf_c_model = RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=-1)\n",
    "rf_c_search = RandomizedSearchCV(rf_c_model, rf_c_params, n_iter=20, cv=5, scoring='f1',\n",
    "                                  random_state=RANDOM_STATE, n_jobs=-1, verbose=0)\n",
    "rf_c_search.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "rf_c_best = rf_c_search.best_estimator_\n",
    "rf_c_pred = rf_c_best.predict(X_test_c_scaled)\n",
    "rf_c_prob = rf_c_best.predict_proba(X_test_c_scaled)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "rf_c_acc = accuracy_score(y_test_c, rf_c_pred)\n",
    "rf_c_prec = precision_score(y_test_c, rf_c_pred)\n",
    "rf_c_rec = recall_score(y_test_c, rf_c_pred)\n",
    "rf_c_f1 = f1_score(y_test_c, rf_c_pred)\n",
    "rf_c_auc = roc_auc_score(y_test_c, rf_c_prob)\n",
    "\n",
    "print(f\"\\n‚úÖ Random Forest Classifier Results:\")\n",
    "print(f\"   Best Params: {rf_c_search.best_params_}\")\n",
    "print(f\"   Accuracy:  {rf_c_acc:.4f}\")\n",
    "print(f\"   Precision: {rf_c_prec:.4f}\")\n",
    "print(f\"   Recall:    {rf_c_rec:.4f}\")\n",
    "print(f\"   F1-Score:  {rf_c_f1:.4f}\")\n",
    "print(f\"   AUC-ROC:   {rf_c_auc:.4f}\")\n",
    "print(f\"\\nClassification Report:\\n{classification_report(y_test_c, rf_c_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1670194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Random Forest Classifier - Visualization Charts\n",
    "# ============================================================\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_rf_c = confusion_matrix(y_test_c, rf_c_pred)\n",
    "sns.heatmap(cm_rf_c, annot=True, fmt='d', cmap='Greens', ax=axes[0,0],\n",
    "            xticklabels=['No Churn', 'Churn'], yticklabels=['No Churn', 'Churn'])\n",
    "axes[0,0].set_title('Random Forest: Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "axes[0,0].set_xlabel('Predicted')\n",
    "axes[0,0].set_ylabel('Actual')\n",
    "\n",
    "# ROC Curve\n",
    "fpr_rf_c, tpr_rf_c, _ = roc_curve(y_test_c, rf_c_prob)\n",
    "axes[0,1].plot(fpr_rf_c, tpr_rf_c, color='green', linewidth=2, label=f'AUC = {rf_c_auc:.4f}')\n",
    "axes[0,1].plot([0, 1], [0, 1], 'k--', linewidth=1)\n",
    "axes[0,1].set_title('Random Forest: ROC Curve', fontsize=14, fontweight='bold')\n",
    "axes[0,1].set_xlabel('False Positive Rate')\n",
    "axes[0,1].set_ylabel('True Positive Rate')\n",
    "axes[0,1].legend(fontsize=12)\n",
    "axes[0,1].fill_between(fpr_rf_c, tpr_rf_c, alpha=0.1, color='green')\n",
    "\n",
    "# Precision-Recall Curve\n",
    "prec_rf_c, rec_rf_c, _ = precision_recall_curve(y_test_c, rf_c_prob)\n",
    "pr_auc_rf_c = auc(rec_rf_c, prec_rf_c)\n",
    "axes[1,0].plot(rec_rf_c, prec_rf_c, color='green', linewidth=2, label=f'PR AUC = {pr_auc_rf_c:.4f}')\n",
    "axes[1,0].set_title('Random Forest: Precision-Recall Curve', fontsize=14, fontweight='bold')\n",
    "axes[1,0].set_xlabel('Recall')\n",
    "axes[1,0].set_ylabel('Precision')\n",
    "axes[1,0].legend(fontsize=12)\n",
    "\n",
    "# Feature Importance\n",
    "rf_c_imp = pd.Series(rf_c_best.feature_importances_, index=churn_feature_names).sort_values(ascending=True)\n",
    "rf_c_imp.tail(15).plot(kind='barh', ax=axes[1,1], color='green', edgecolor='black')\n",
    "axes[1,1].set_title('Random Forest: Top 15 Feature Importances', fontsize=14, fontweight='bold')\n",
    "axes[1,1].set_xlabel('Importance')\n",
    "\n",
    "plt.tight_layout()\n",
    "save_chart(fig, '06_rf_churn_results.png', 'customer_churn')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ba7918",
   "metadata": {},
   "source": [
    "## 14. Customer Churn - XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e6eb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 14. XGBoost Classifier for Churn\n",
    "# ============================================================\n",
    "print(\"üöÄ Training XGBoost Classifier...\")\n",
    "\n",
    "# Calculate scale_pos_weight\n",
    "n_neg = (y_train_c == 0).sum()\n",
    "n_pos = (y_train_c == 1).sum()\n",
    "scale_pos = n_neg / n_pos\n",
    "\n",
    "xgb_c_model = xgb.XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    scale_pos_weight=scale_pos,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=1.0,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    "    eval_metric='logloss',\n",
    "    verbosity=0\n",
    ")\n",
    "\n",
    "# Split for early stopping\n",
    "X_tr_c, X_val_c, y_tr_c, y_val_c = train_test_split(\n",
    "    X_train_smote, y_train_smote, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "xgb_c_model.fit(\n",
    "    X_tr_c, y_tr_c,\n",
    "    eval_set=[(X_tr_c, y_tr_c), (X_val_c, y_val_c)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "xgb_c_pred = xgb_c_model.predict(X_test_c_scaled)\n",
    "xgb_c_prob = xgb_c_model.predict_proba(X_test_c_scaled)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "xgb_c_acc = accuracy_score(y_test_c, xgb_c_pred)\n",
    "xgb_c_prec = precision_score(y_test_c, xgb_c_pred)\n",
    "xgb_c_rec = recall_score(y_test_c, xgb_c_pred)\n",
    "xgb_c_f1 = f1_score(y_test_c, xgb_c_pred)\n",
    "xgb_c_auc = roc_auc_score(y_test_c, xgb_c_prob)\n",
    "\n",
    "print(f\"\\n‚úÖ XGBoost Classifier Results:\")\n",
    "print(f\"   Accuracy:  {xgb_c_acc:.4f}\")\n",
    "print(f\"   Precision: {xgb_c_prec:.4f}\")\n",
    "print(f\"   Recall:    {xgb_c_rec:.4f}\")\n",
    "print(f\"   F1-Score:  {xgb_c_f1:.4f}\")\n",
    "print(f\"   AUC-ROC:   {xgb_c_auc:.4f}\")\n",
    "print(f\"\\nClassification Report:\\n{classification_report(y_test_c, xgb_c_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16310616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# XGBoost Classifier - Visualization Charts\n",
    "# ============================================================\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_xgb_c = confusion_matrix(y_test_c, xgb_c_pred)\n",
    "sns.heatmap(cm_xgb_c, annot=True, fmt='d', cmap='Oranges', ax=axes[0,0],\n",
    "            xticklabels=['No Churn', 'Churn'], yticklabels=['No Churn', 'Churn'])\n",
    "axes[0,0].set_title('XGBoost: Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "axes[0,0].set_xlabel('Predicted')\n",
    "axes[0,0].set_ylabel('Actual')\n",
    "\n",
    "# ROC Curve\n",
    "fpr_xgb_c, tpr_xgb_c, _ = roc_curve(y_test_c, xgb_c_prob)\n",
    "axes[0,1].plot(fpr_xgb_c, tpr_xgb_c, color='orange', linewidth=2, label=f'AUC = {xgb_c_auc:.4f}')\n",
    "axes[0,1].plot([0, 1], [0, 1], 'k--', linewidth=1)\n",
    "axes[0,1].set_title('XGBoost: ROC Curve', fontsize=14, fontweight='bold')\n",
    "axes[0,1].set_xlabel('False Positive Rate')\n",
    "axes[0,1].set_ylabel('True Positive Rate')\n",
    "axes[0,1].legend(fontsize=12)\n",
    "axes[0,1].fill_between(fpr_xgb_c, tpr_xgb_c, alpha=0.1, color='orange')\n",
    "\n",
    "# Precision-Recall Curve\n",
    "prec_xgb_c, rec_xgb_c, _ = precision_recall_curve(y_test_c, xgb_c_prob)\n",
    "pr_auc_xgb_c = auc(rec_xgb_c, prec_xgb_c)\n",
    "axes[1,0].plot(rec_xgb_c, prec_xgb_c, color='orange', linewidth=2, label=f'PR AUC = {pr_auc_xgb_c:.4f}')\n",
    "axes[1,0].set_title('XGBoost: Precision-Recall Curve', fontsize=14, fontweight='bold')\n",
    "axes[1,0].set_xlabel('Recall')\n",
    "axes[1,0].set_ylabel('Precision')\n",
    "axes[1,0].legend(fontsize=12)\n",
    "\n",
    "# Feature Importance (Gain-based)\n",
    "xgb_c_imp = pd.Series(xgb_c_model.feature_importances_, index=churn_feature_names).sort_values(ascending=True)\n",
    "xgb_c_imp.tail(15).plot(kind='barh', ax=axes[1,1], color='orange', edgecolor='black')\n",
    "axes[1,1].set_title('XGBoost: Top 15 Feature Importances', fontsize=14, fontweight='bold')\n",
    "axes[1,1].set_xlabel('Importance')\n",
    "\n",
    "plt.tight_layout()\n",
    "save_chart(fig, '07_xgb_churn_results.png', 'customer_churn')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965fe657",
   "metadata": {},
   "source": [
    "## 15. Customer Churn - Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164c31bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 15. Deep Neural Network for Churn Prediction\n",
    "# ============================================================\n",
    "print(\"üß† Training Deep Neural Network...\")\n",
    "\n",
    "n_features = X_train_smote.shape[1]\n",
    "\n",
    "dnn_model = Sequential([\n",
    "    Input(shape=(n_features,)),\n",
    "    Dense(128, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "dnn_model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Class weights\n",
    "class_weights = {0: 1.0, 1: scale_pos}\n",
    "\n",
    "dnn_callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=0)\n",
    "]\n",
    "\n",
    "dnn_history = dnn_model.fit(\n",
    "    X_train_smote, y_train_smote,\n",
    "    validation_data=(X_test_c_scaled, y_test_c),\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    callbacks=dnn_callbacks,\n",
    "    class_weight=class_weights,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Predict with optimal threshold using ROC\n",
    "dnn_prob = dnn_model.predict(X_test_c_scaled, verbose=0).flatten()\n",
    "fpr_dnn, tpr_dnn, thresholds_dnn = roc_curve(y_test_c, dnn_prob)\n",
    "optimal_idx = np.argmax(tpr_dnn - fpr_dnn)\n",
    "optimal_threshold = thresholds_dnn[optimal_idx]\n",
    "dnn_pred = (dnn_prob >= optimal_threshold).astype(int)\n",
    "\n",
    "# Metrics\n",
    "dnn_acc = accuracy_score(y_test_c, dnn_pred)\n",
    "dnn_prec = precision_score(y_test_c, dnn_pred)\n",
    "dnn_rec = recall_score(y_test_c, dnn_pred)\n",
    "dnn_f1 = f1_score(y_test_c, dnn_pred)\n",
    "dnn_auc = roc_auc_score(y_test_c, dnn_prob)\n",
    "\n",
    "print(f\"\\n‚úÖ Deep Neural Network Results:\")\n",
    "print(f\"   Optimal Threshold: {optimal_threshold:.4f}\")\n",
    "print(f\"   Accuracy:  {dnn_acc:.4f}\")\n",
    "print(f\"   Precision: {dnn_prec:.4f}\")\n",
    "print(f\"   Recall:    {dnn_rec:.4f}\")\n",
    "print(f\"   F1-Score:  {dnn_f1:.4f}\")\n",
    "print(f\"   AUC-ROC:   {dnn_auc:.4f}\")\n",
    "print(f\"\\nClassification Report:\\n{classification_report(y_test_c, dnn_pred)}\")\n",
    "print(f\"\\nModel Summary:\")\n",
    "dnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a27638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Deep Neural Network - Visualization Charts\n",
    "# ============================================================\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
    "\n",
    "# Training/Validation Accuracy\n",
    "axes[0,0].plot(dnn_history.history['accuracy'], label='Training Accuracy', color='steelblue', linewidth=2)\n",
    "axes[0,0].plot(dnn_history.history['val_accuracy'], label='Validation Accuracy', color='red', linewidth=2)\n",
    "axes[0,0].set_title('DNN: Training & Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0,0].set_xlabel('Epoch')\n",
    "axes[0,0].set_ylabel('Accuracy')\n",
    "axes[0,0].legend(fontsize=12)\n",
    "\n",
    "# Training/Validation Loss\n",
    "axes[0,1].plot(dnn_history.history['loss'], label='Training Loss', color='steelblue', linewidth=2)\n",
    "axes[0,1].plot(dnn_history.history['val_loss'], label='Validation Loss', color='red', linewidth=2)\n",
    "axes[0,1].set_title('DNN: Training & Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[0,1].set_xlabel('Epoch')\n",
    "axes[0,1].set_ylabel('Loss')\n",
    "axes[0,1].legend(fontsize=12)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_dnn = confusion_matrix(y_test_c, dnn_pred)\n",
    "sns.heatmap(cm_dnn, annot=True, fmt='d', cmap='Purples', ax=axes[1,0],\n",
    "            xticklabels=['No Churn', 'Churn'], yticklabels=['No Churn', 'Churn'])\n",
    "axes[1,0].set_title('DNN: Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "axes[1,0].set_xlabel('Predicted')\n",
    "axes[1,0].set_ylabel('Actual')\n",
    "\n",
    "# ROC Curve\n",
    "axes[1,1].plot(fpr_dnn, tpr_dnn, color='purple', linewidth=2, label=f'AUC = {dnn_auc:.4f}')\n",
    "axes[1,1].plot([0, 1], [0, 1], 'k--', linewidth=1)\n",
    "axes[1,1].scatter(fpr_dnn[optimal_idx], tpr_dnn[optimal_idx], color='red', s=100, zorder=5,\n",
    "                  label=f'Optimal Threshold = {optimal_threshold:.4f}')\n",
    "axes[1,1].set_title('DNN: ROC Curve with Optimal Threshold', fontsize=14, fontweight='bold')\n",
    "axes[1,1].set_xlabel('False Positive Rate')\n",
    "axes[1,1].set_ylabel('True Positive Rate')\n",
    "axes[1,1].legend(fontsize=11)\n",
    "axes[1,1].fill_between(fpr_dnn, tpr_dnn, alpha=0.1, color='purple')\n",
    "\n",
    "plt.tight_layout()\n",
    "save_chart(fig, '08_dnn_churn_results.png', 'customer_churn')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0410e3e8",
   "metadata": {},
   "source": [
    "## 16. Churn Model Comparison & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106a768d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 16. Churn Model Comparison\n",
    "# ============================================================\n",
    "churn_comparison = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'Random Forest', 'XGBoost', 'Deep Neural Network'],\n",
    "    'Accuracy': [lr_acc, rf_c_acc, xgb_c_acc, dnn_acc],\n",
    "    'Precision': [lr_prec, rf_c_prec, xgb_c_prec, dnn_prec],\n",
    "    'Recall': [lr_rec, rf_c_rec, xgb_c_rec, dnn_rec],\n",
    "    'F1-Score': [lr_f1, rf_c_f1, xgb_c_f1, dnn_f1],\n",
    "    'AUC-ROC': [lr_auc, rf_c_auc, xgb_c_auc, dnn_auc]\n",
    "})\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üë• CUSTOMER CHURN - MODEL COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print(churn_comparison.to_string(index=False))\n",
    "print(f\"\\nüèÜ Best Model by F1: {churn_comparison.loc[churn_comparison['F1-Score'].idxmax(), 'Model']}\")\n",
    "print(f\"üèÜ Best Model by AUC: {churn_comparison.loc[churn_comparison['AUC-ROC'].idxmax(), 'Model']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5441c8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Churn Model Comparison - Visualization Charts\n",
    "# ============================================================\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 14))\n",
    "\n",
    "# Grouped Bar Chart - All Metrics\n",
    "metrics_cols = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC']\n",
    "x = np.arange(len(metrics_cols))\n",
    "width = 0.2\n",
    "colors_churn = ['steelblue', 'green', 'orange', 'purple']\n",
    "for i, model in enumerate(churn_comparison['Model']):\n",
    "    values = [churn_comparison.loc[i, m] for m in metrics_cols]\n",
    "    axes[0,0].bar(x + i*width, values, width, label=model, color=colors_churn[i], edgecolor='black', alpha=0.8)\n",
    "axes[0,0].set_xticks(x + 1.5*width)\n",
    "axes[0,0].set_xticklabels(metrics_cols, rotation=15)\n",
    "axes[0,0].set_title('Churn Models: All Metrics Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0,0].set_ylabel('Score')\n",
    "axes[0,0].legend(fontsize=9, loc='lower right')\n",
    "axes[0,0].set_ylim(0, 1.05)\n",
    "\n",
    "# All ROC Curves Overlay\n",
    "axes[0,1].plot(fpr_lr, tpr_lr, color='steelblue', linewidth=2, label=f'LR (AUC={lr_auc:.3f})')\n",
    "axes[0,1].plot(fpr_rf_c, tpr_rf_c, color='green', linewidth=2, label=f'RF (AUC={rf_c_auc:.3f})')\n",
    "axes[0,1].plot(fpr_xgb_c, tpr_xgb_c, color='orange', linewidth=2, label=f'XGB (AUC={xgb_c_auc:.3f})')\n",
    "axes[0,1].plot(fpr_dnn, tpr_dnn, color='purple', linewidth=2, label=f'DNN (AUC={dnn_auc:.3f})')\n",
    "axes[0,1].plot([0, 1], [0, 1], 'k--', linewidth=1)\n",
    "axes[0,1].set_title('All Models: ROC Curves', fontsize=14, fontweight='bold')\n",
    "axes[0,1].set_xlabel('False Positive Rate')\n",
    "axes[0,1].set_ylabel('True Positive Rate')\n",
    "axes[0,1].legend(fontsize=11)\n",
    "\n",
    "# All Precision-Recall Curves\n",
    "prec_dnn, rec_dnn, _ = precision_recall_curve(y_test_c, dnn_prob)\n",
    "axes[1,0].plot(rec_lr, prec_lr, color='steelblue', linewidth=2, label=f'LR')\n",
    "axes[1,0].plot(rec_rf_c, prec_rf_c, color='green', linewidth=2, label=f'RF')\n",
    "axes[1,0].plot(rec_xgb_c, prec_xgb_c, color='orange', linewidth=2, label=f'XGB')\n",
    "axes[1,0].plot(rec_dnn, prec_dnn, color='purple', linewidth=2, label=f'DNN')\n",
    "axes[1,0].set_title('All Models: Precision-Recall Curves', fontsize=14, fontweight='bold')\n",
    "axes[1,0].set_xlabel('Recall')\n",
    "axes[1,0].set_ylabel('Precision')\n",
    "axes[1,0].legend(fontsize=11)\n",
    "\n",
    "# Radar Chart for Churn Models\n",
    "categories_c = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC']\n",
    "angles_c = np.linspace(0, 2 * np.pi, len(categories_c), endpoint=False).tolist()\n",
    "angles_c += angles_c[:1]\n",
    "\n",
    "ax_radar_c = plt.subplot(2, 2, 4, polar=True)\n",
    "for i, model in enumerate(churn_comparison['Model']):\n",
    "    values = [churn_comparison.loc[i, m] for m in categories_c]\n",
    "    values += values[:1]\n",
    "    ax_radar_c.plot(angles_c, values, 'o-', linewidth=2, label=model, color=colors_churn[i])\n",
    "    ax_radar_c.fill(angles_c, values, alpha=0.05, color=colors_churn[i])\n",
    "ax_radar_c.set_xticks(angles_c[:-1])\n",
    "ax_radar_c.set_xticklabels(categories_c, fontsize=9)\n",
    "ax_radar_c.set_title('Churn Models: Radar Comparison', fontsize=14, fontweight='bold', pad=20)\n",
    "ax_radar_c.legend(fontsize=8, loc='upper right', bbox_to_anchor=(1.4, 1.1))\n",
    "\n",
    "plt.tight_layout()\n",
    "save_chart(fig, '11_churn_model_comparison.png', 'comparison')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d04c2a6",
   "metadata": {},
   "source": [
    "## 17. Business Insights & Recommendations Dashboard\n",
    "\n",
    "Executive-level dashboards summarizing key findings for both projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942388fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 17. Business Insights Dashboard - Sales Forecasting\n",
    "# ============================================================\n",
    "fig = plt.figure(figsize=(24, 16))\n",
    "fig.suptitle('üìä SALES FORECASTING - EXECUTIVE DASHBOARD', fontsize=20, fontweight='bold', y=0.98)\n",
    "\n",
    "# Panel 1: Sales Trend with Forecast\n",
    "ax1 = fig.add_subplot(2, 3, 1)\n",
    "ax1.plot(dates_test[-60:], y_test[-60:], label='Actual', color='steelblue', linewidth=2)\n",
    "ax1.plot(dates_test[-60:], rf_pred[-60:], label='RF Forecast', color='red', linewidth=1.5, alpha=0.7)\n",
    "ax1.plot(dates_test[-60:], xgb_pred[-60:], label='XGB Forecast', color='orange', linewidth=1.5, alpha=0.7)\n",
    "ax1.fill_between(dates_test[-60:], \n",
    "                  np.minimum(rf_pred[-60:], xgb_pred[-60:]),\n",
    "                  np.maximum(rf_pred[-60:], xgb_pred[-60:]), alpha=0.15, color='gray', label='Confidence Band')\n",
    "ax1.set_title('Sales Forecast (Last 60 Days)', fontsize=12, fontweight='bold')\n",
    "ax1.legend(fontsize=8)\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Panel 2: Monthly Sales Pattern\n",
    "ax2 = fig.add_subplot(2, 3, 2)\n",
    "monthly_avg = sales_df.groupby('Month')['Sales'].mean()\n",
    "colors_month = plt.cm.RdYlGn(np.linspace(0.2, 0.9, 12))\n",
    "ax2.bar(range(1, 13), monthly_avg.values, color=colors_month, edgecolor='black')\n",
    "ax2.set_title('Average Sales by Month', fontsize=12, fontweight='bold')\n",
    "ax2.set_xlabel('Month')\n",
    "ax2.set_ylabel('Avg Sales ($)')\n",
    "ax2.set_xticks(range(1, 13))\n",
    "peak_month = monthly_avg.idxmax()\n",
    "ax2.annotate(f'Peak: Month {peak_month}', xy=(peak_month, monthly_avg.max()),\n",
    "             xytext=(peak_month+1, monthly_avg.max()*1.1),\n",
    "             arrowprops=dict(arrowstyle='->', color='red'), fontsize=10, color='red', fontweight='bold')\n",
    "\n",
    "# Panel 3: Category Revenue\n",
    "ax3 = fig.add_subplot(2, 3, 3)\n",
    "cat_sales = sales_df.groupby('Category')['Sales'].sum().sort_values()\n",
    "ax3.barh(cat_sales.index, cat_sales.values, color=['#FF6B6B', '#4ECDC4', '#45B7D1'], edgecolor='black')\n",
    "ax3.set_title('Total Revenue by Category', fontsize=12, fontweight='bold')\n",
    "ax3.set_xlabel('Total Sales ($)')\n",
    "for i, v in enumerate(cat_sales.values):\n",
    "    ax3.text(v + 100, i, f'${v:,.0f}', va='center', fontweight='bold', fontsize=10)\n",
    "\n",
    "# Panel 4: Regional Sales\n",
    "ax4 = fig.add_subplot(2, 3, 4)\n",
    "region_sales = sales_df.groupby('Region')['Sales'].sum().sort_values()\n",
    "ax4.pie(region_sales.values, labels=region_sales.index, autopct='%1.1f%%',\n",
    "        colors=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'], startangle=90,\n",
    "        textprops={'fontsize': 11})\n",
    "ax4.set_title('Sales Distribution by Region', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Panel 5: Model Performance Summary\n",
    "ax5 = fig.add_subplot(2, 3, 5)\n",
    "ax5.axis('off')\n",
    "table_data = []\n",
    "for _, row in sales_comparison.iterrows():\n",
    "    table_data.append([row['Model'], f\"${row['MAE']:.0f}\", f\"${row['RMSE']:.0f}\", f\"{row['R¬≤']:.3f}\"])\n",
    "table = ax5.table(cellText=table_data, colLabels=['Model', 'MAE', 'RMSE', 'R¬≤'],\n",
    "                   loc='center', cellLoc='center')\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(11)\n",
    "table.scale(1.2, 1.8)\n",
    "for (i, j), cell in table.get_celld().items():\n",
    "    if i == 0:\n",
    "        cell.set_facecolor('#4ECDC4')\n",
    "        cell.set_text_props(color='white', fontweight='bold')\n",
    "ax5.set_title('Sales Model Performance Summary', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Panel 6: Key Metrics KPIs\n",
    "ax6 = fig.add_subplot(2, 3, 6)\n",
    "ax6.axis('off')\n",
    "total_sales = sales_df['Sales'].sum()\n",
    "avg_order = sales_df['Sales'].mean()\n",
    "total_orders = len(sales_df)\n",
    "best_model = sales_comparison.loc[sales_comparison['R¬≤'].idxmax(), 'Model']\n",
    "kpi_text = f\"\"\"\n",
    "üìà KEY BUSINESS METRICS\n",
    "\n",
    "Total Revenue: ${total_sales:,.0f}\n",
    "Average Order Value: ${avg_order:,.2f}\n",
    "Total Orders: {total_orders:,}\n",
    "Best Forecasting Model: {best_model}\n",
    "Best R¬≤ Score: {sales_comparison['R¬≤'].max():.4f}\n",
    "Forecast MAPE: {sales_comparison['MAPE (%)'].min():.1f}%\n",
    "Peak Sales Month: {peak_month}\n",
    "\"\"\"\n",
    "ax6.text(0.1, 0.5, kpi_text, transform=ax6.transAxes, fontsize=13,\n",
    "         verticalalignment='center', fontfamily='monospace',\n",
    "         bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "save_chart(fig, '01_sales_executive_dashboard.png', 'dashboard')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4cad5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 17b. Business Insights Dashboard - Customer Churn\n",
    "# ============================================================\n",
    "fig = plt.figure(figsize=(24, 16))\n",
    "fig.suptitle('üë• CUSTOMER CHURN PREDICTION - EXECUTIVE DASHBOARD', fontsize=20, fontweight='bold', y=0.98)\n",
    "\n",
    "# Panel 1: Churn Risk Distribution\n",
    "ax1 = fig.add_subplot(2, 3, 1)\n",
    "# Use the best model's probabilities for risk segmentation\n",
    "best_prob = xgb_c_prob  # XGBoost typically best\n",
    "risk_labels = pd.cut(best_prob, bins=[0, 0.3, 0.6, 1.0], labels=['Low Risk', 'Medium Risk', 'High Risk'])\n",
    "risk_counts = risk_labels.value_counts()\n",
    "ax1.pie(risk_counts.values, labels=risk_counts.index, autopct='%1.1f%%',\n",
    "        colors=['#4ECDC4', '#FFB347', '#FF6B6B'], startangle=90, explode=[0, 0.05, 0.1],\n",
    "        textprops={'fontsize': 12, 'fontweight': 'bold'})\n",
    "ax1.set_title('Customer Risk Segmentation', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Panel 2: Churn Probability Distribution\n",
    "ax2 = fig.add_subplot(2, 3, 2)\n",
    "ax2.hist(best_prob, bins=30, color='steelblue', edgecolor='black', alpha=0.7, label='All Customers')\n",
    "ax2.axvline(x=0.5, color='red', linestyle='--', linewidth=2, label='Default Threshold')\n",
    "ax2.axvline(x=optimal_threshold, color='green', linestyle='--', linewidth=2, label=f'Optimal ({optimal_threshold:.2f})')\n",
    "ax2.set_title('Churn Probability Distribution', fontsize=13, fontweight='bold')\n",
    "ax2.set_xlabel('Churn Probability')\n",
    "ax2.set_ylabel('Count')\n",
    "ax2.legend(fontsize=10)\n",
    "\n",
    "# Panel 3: Revenue at Risk\n",
    "ax3 = fig.add_subplot(2, 3, 3)\n",
    "churn_test_df = churn_df.iloc[X_test_c.index].copy()\n",
    "churn_test_df['ChurnProb'] = best_prob\n",
    "churn_test_df['RiskLevel'] = risk_labels.values\n",
    "revenue_at_risk = churn_test_df.groupby('RiskLevel')['MonthlyCharges'].sum()\n",
    "ax3.bar(revenue_at_risk.index, revenue_at_risk.values, color=['#4ECDC4', '#FFB347', '#FF6B6B'], edgecolor='black')\n",
    "ax3.set_title('Monthly Revenue at Risk by Segment', fontsize=13, fontweight='bold')\n",
    "ax3.set_ylabel('Monthly Revenue ($)')\n",
    "for i, v in enumerate(revenue_at_risk.values):\n",
    "    ax3.text(i, v + 50, f'${v:,.0f}', ha='center', fontweight='bold', fontsize=11)\n",
    "\n",
    "# Panel 4: Top Churn Factors\n",
    "ax4 = fig.add_subplot(2, 3, 4)\n",
    "top_features = xgb_c_imp.tail(10)\n",
    "ax4.barh(top_features.index, top_features.values, color=plt.cm.RdYlGn_r(np.linspace(0.2, 0.9, 10)), edgecolor='black')\n",
    "ax4.set_title('Top 10 Churn Drivers', fontsize=13, fontweight='bold')\n",
    "ax4.set_xlabel('Feature Importance')\n",
    "\n",
    "# Panel 5: Model Performance Summary\n",
    "ax5 = fig.add_subplot(2, 3, 5)\n",
    "ax5.axis('off')\n",
    "table_data_c = []\n",
    "for _, row in churn_comparison.iterrows():\n",
    "    table_data_c.append([row['Model'], f\"{row['Accuracy']:.3f}\", f\"{row['F1-Score']:.3f}\", f\"{row['AUC-ROC']:.3f}\"])\n",
    "table_c = ax5.table(cellText=table_data_c, colLabels=['Model', 'Accuracy', 'F1', 'AUC'],\n",
    "                     loc='center', cellLoc='center')\n",
    "table_c.auto_set_font_size(False)\n",
    "table_c.set_fontsize(11)\n",
    "table_c.scale(1.2, 1.8)\n",
    "for (i, j), cell in table_c.get_celld().items():\n",
    "    if i == 0:\n",
    "        cell.set_facecolor('#FF6B6B')\n",
    "        cell.set_text_props(color='white', fontweight='bold')\n",
    "ax5.set_title('Churn Model Performance Summary', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Panel 6: Key Metrics KPIs\n",
    "ax6 = fig.add_subplot(2, 3, 6)\n",
    "ax6.axis('off')\n",
    "total_customers = len(churn_df)\n",
    "churn_rate = (churn_df['Churn'] == 'Yes').mean() * 100\n",
    "high_risk_count = (risk_labels == 'High Risk').sum()\n",
    "best_churn_model = churn_comparison.loc[churn_comparison['AUC-ROC'].idxmax(), 'Model']\n",
    "avg_monthly = churn_df['MonthlyCharges'].mean()\n",
    "kpi_text_c = f\"\"\"\n",
    "üë• KEY CHURN METRICS\n",
    "\n",
    "Total Customers: {total_customers:,}\n",
    "Overall Churn Rate: {churn_rate:.1f}%\n",
    "High-Risk Customers (Test): {high_risk_count}\n",
    "Best Model: {best_churn_model}\n",
    "Best AUC-ROC: {churn_comparison['AUC-ROC'].max():.4f}\n",
    "Best F1-Score: {churn_comparison['F1-Score'].max():.4f}\n",
    "Avg Monthly Revenue/Customer: ${avg_monthly:.2f}\n",
    "\"\"\"\n",
    "ax6.text(0.1, 0.5, kpi_text_c, transform=ax6.transAxes, fontsize=13,\n",
    "         verticalalignment='center', fontfamily='monospace',\n",
    "         bbox=dict(boxstyle='round', facecolor='mistyrose', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "save_chart(fig, '02_churn_executive_dashboard.png', 'dashboard')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e164ec9",
   "metadata": {},
   "source": [
    "## 18. Feature Importance Analysis - Permutation Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57009813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 18. Permutation Importance for Both Projects\n",
    "# ============================================================\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Sales - Permutation Importance (RF)\n",
    "perm_imp_sales = permutation_importance(rf_best, X_test_scaled, y_test, n_repeats=10, \n",
    "                                         random_state=RANDOM_STATE, n_jobs=-1)\n",
    "perm_sales_df = pd.Series(perm_imp_sales.importances_mean, index=feature_cols).sort_values(ascending=True)\n",
    "perm_sales_df.tail(15).plot(kind='barh', ax=axes[0], color='steelblue', edgecolor='black')\n",
    "axes[0].set_title('Sales RF: Permutation Importance (Top 15)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Mean Importance')\n",
    "\n",
    "# Churn - Permutation Importance (XGBoost)\n",
    "perm_imp_churn = permutation_importance(xgb_c_model, X_test_c_scaled, y_test_c, n_repeats=10,\n",
    "                                         random_state=RANDOM_STATE, n_jobs=-1, scoring='f1')\n",
    "perm_churn_df = pd.Series(perm_imp_churn.importances_mean, index=churn_feature_names).sort_values(ascending=True)\n",
    "perm_churn_df.tail(15).plot(kind='barh', ax=axes[1], color='orange', edgecolor='black')\n",
    "axes[1].set_title('Churn XGB: Permutation Importance (Top 15)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Mean Importance')\n",
    "\n",
    "plt.tight_layout()\n",
    "save_chart(fig, '12_permutation_importance.png', 'comparison')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607473e5",
   "metadata": {},
   "source": [
    "## 19. Export Results & Final Chart Gallery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c6ef28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 19. Export All Results to CSV\n",
    "# ============================================================\n",
    "\n",
    "# Save model comparison results\n",
    "sales_comparison.to_csv('charts/sales_model_comparison.csv', index=False)\n",
    "churn_comparison.to_csv('charts/churn_model_comparison.csv', index=False)\n",
    "\n",
    "# Save sales predictions\n",
    "sales_pred_df = pd.DataFrame({\n",
    "    'Date': dates_test,\n",
    "    'Actual_Sales': y_test,\n",
    "    'RF_Predicted': rf_pred,\n",
    "    'XGB_Predicted': xgb_pred\n",
    "})\n",
    "sales_pred_df.to_csv('charts/sales_predictions.csv', index=False)\n",
    "\n",
    "# Save churn predictions\n",
    "churn_pred_df = pd.DataFrame({\n",
    "    'Actual_Churn': y_test_c.values,\n",
    "    'LR_Predicted': lr_pred,\n",
    "    'RF_Predicted': rf_c_pred,\n",
    "    'XGB_Predicted': xgb_c_pred,\n",
    "    'DNN_Predicted': dnn_pred,\n",
    "    'LR_Probability': lr_prob,\n",
    "    'RF_Probability': rf_c_prob,\n",
    "    'XGB_Probability': xgb_c_prob,\n",
    "    'DNN_Probability': dnn_prob\n",
    "})\n",
    "churn_pred_df.to_csv('charts/churn_predictions.csv', index=False)\n",
    "\n",
    "# Save feature importances\n",
    "pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'RF_Sales_Importance': rf_best.feature_importances_,\n",
    "    'XGB_Sales_Importance': xgb_model.feature_importances_\n",
    "}).to_csv('charts/sales_feature_importances.csv', index=False)\n",
    "\n",
    "pd.DataFrame({\n",
    "    'Feature': churn_feature_names,\n",
    "    'RF_Churn_Importance': rf_c_best.feature_importances_,\n",
    "    'XGB_Churn_Importance': xgb_c_model.feature_importances_\n",
    "}).to_csv('charts/churn_feature_importances.csv', index=False)\n",
    "\n",
    "print(\"‚úÖ All results exported to CSV files!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f79a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Final Directory Listing & Summary\n",
    "# ============================================================\n",
    "print(\"=\" * 80)\n",
    "print(\"üìÅ COMPLETE CHART DIRECTORY LISTING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "total_files = 0\n",
    "total_size = 0\n",
    "for root, dirs, files in os.walk(CHARTS_DIR):\n",
    "    level = root.replace(CHARTS_DIR, '').count(os.sep)\n",
    "    indent = ' ' * 2 * level\n",
    "    print(f'{indent}üìÅ {os.path.basename(root)}/')\n",
    "    subindent = ' ' * 2 * (level + 1)\n",
    "    for file in sorted(files):\n",
    "        filepath = os.path.join(root, file)\n",
    "        size = os.path.getsize(filepath)\n",
    "        total_size += size\n",
    "        total_files += 1\n",
    "        print(f'{subindent}üìÑ {file} ({size/1024:.1f} KB)')\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(f\"üìä Total charts saved: {total_files}\")\n",
    "print(f\"üíæ Total size: {total_size/1024/1024:.2f} MB\")\n",
    "print(f\"{'=' * 80}\")\n",
    "\n",
    "print(f\"\"\"\n",
    "{'=' * 80}\n",
    "üèÜ FINAL SUMMARY - AI-DRIVEN PREDICTIVE ANALYTICS\n",
    "{'=' * 80}\n",
    "\n",
    "üìä PROJECT 1: SALES FORECASTING\n",
    "{'‚îÄ' * 40}\n",
    "‚Ä¢ Dataset: {len(sales_df)} records, {sales_df['Order Date'].min().strftime('%Y-%m-%d')} to {sales_df['Order Date'].max().strftime('%Y-%m-%d')}\n",
    "‚Ä¢ Models Trained: Random Forest, XGBoost, LSTM\n",
    "‚Ä¢ Best Model: {sales_comparison.loc[sales_comparison['R¬≤'].idxmax(), 'Model']}\n",
    "  - R¬≤ Score: {sales_comparison['R¬≤'].max():.4f}\n",
    "  - MAE: ${sales_comparison.loc[sales_comparison['R¬≤'].idxmax(), 'MAE']:.2f}\n",
    "  - RMSE: ${sales_comparison.loc[sales_comparison['R¬≤'].idxmax(), 'RMSE']:.2f}\n",
    "‚Ä¢ Key Drivers: Lag features, rolling averages, seasonal patterns\n",
    "\n",
    "üë• PROJECT 2: CUSTOMER CHURN PREDICTION\n",
    "{'‚îÄ' * 40}\n",
    "‚Ä¢ Dataset: {len(churn_df)} customers\n",
    "‚Ä¢ Overall Churn Rate: {(churn_df['Churn'] == 'Yes').mean()*100:.1f}%\n",
    "‚Ä¢ Models Trained: Logistic Regression, Random Forest, XGBoost, DNN\n",
    "‚Ä¢ Best Model: {churn_comparison.loc[churn_comparison['AUC-ROC'].idxmax(), 'Model']}\n",
    "  - AUC-ROC: {churn_comparison['AUC-ROC'].max():.4f}\n",
    "  - F1-Score: {churn_comparison.loc[churn_comparison['AUC-ROC'].idxmax(), 'F1-Score']:.4f}\n",
    "  - Recall: {churn_comparison.loc[churn_comparison['AUC-ROC'].idxmax(), 'Recall']:.4f}\n",
    "‚Ä¢ Key Churn Drivers: Contract type, tenure, monthly charges, internet service\n",
    "\n",
    "üí° BUSINESS RECOMMENDATIONS\n",
    "{'‚îÄ' * 40}\n",
    "1. Use the best sales model for monthly revenue forecasting\n",
    "2. Implement churn prediction for proactive customer retention\n",
    "3. Focus on Month-to-Month contract customers (highest churn risk)\n",
    "4. Target high-risk customers with retention offers\n",
    "5. Leverage seasonal patterns for inventory and staffing planning\n",
    "6. Monitor feature importance changes over time for model retraining\n",
    "\n",
    "{'=' * 80}\n",
    "‚úÖ Analysis Complete! All charts saved to 'charts/' directory.\n",
    "{'=' * 80}\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
